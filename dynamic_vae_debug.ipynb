{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "sys.path.append('../..')\n",
    "# os.chdir(\"../tmp/HunyuanVideo\")\n",
    "sys.path.append('..')\n",
    "sys.path.append('.')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "\n",
    "# save_path = os.path.join(output_dir, f\"{file_name}.mp4\")\n",
    "\n",
    "def show_video(videos: torch.Tensor, save_path=None, rescale=True):\n",
    "    for x in videos:\n",
    "        x=x.cpu().float() # c t h w\n",
    "        x=x.permute(1,2,3,0) # t h w c\n",
    "        if rescale:\n",
    "            x = (x + 1.0) / 2.0  # -1,1 -> 0,1\n",
    "        x = torch.clamp(x, 0, 1)\n",
    "        x = (x * 255).numpy().astype(np.uint8)\n",
    "        fig = plt.figure()\n",
    "        im = plt.imshow(x[0,:,:,:])\n",
    "        def init():\n",
    "            im.set_data(x[0,:,:,:])\n",
    "\n",
    "        def animate(i):\n",
    "            im.set_data(x[i,:,:,:])\n",
    "            return im\n",
    "        plt.close()\n",
    "        anim = animation.FuncAnimation(fig, animate, init_func=init, frames=x.shape[0],\n",
    "                                    interval=50)\n",
    "        return HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hyvideo.config\n",
    "print(hyvideo.config.__file__)\n",
    "from hyvideo.config import parse_args\n",
    "string_args=\"\"\"--video-size 720 1280 --video-length 129 --infer-steps 50 --prompt cat. --flow-reverse --use-cpu-offload --save-path ./results\"\"\"\n",
    "string_args=string_args.split(\" \")\n",
    "print(string_args)\n",
    "args = parse_args(string_args=string_args)\n",
    "print(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from loguru import logger\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "from hyvideo.utils.file_utils import save_videos_grid\n",
    "from hyvideo.config import parse_args\n",
    "from hyvideo.inference import HunyuanVideoSampler\n",
    "from hyvideo.modules import load_model\n",
    "from hyvideo.vae import load_vae\n",
    "from hyvideo.vae.autoencoder_kl_causal_3d import AutoencoderKLCausal3D\n",
    "from hyvideo.diffusion.pipelines import HunyuanVideoPipeline\n",
    "\n",
    "# hunyuan_video_sampler = HunyuanVideoSampler.from_pretrained(models_root_path, args=args)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_grad_enabled(False)\n",
    "# model = load_model(\n",
    "#     args,\n",
    "#     in_channels=in_channels,\n",
    "#     out_channels=out_channels,\n",
    "#     factor_kwargs=factor_kwargs,\n",
    "# )\n",
    "\n",
    "t_ops_config_path=\"t_ops_config.json\"\n",
    "\n",
    "vae, _, s_ratio, t_ratio = load_vae(\n",
    "            args.vae,\n",
    "            args.vae_precision,\n",
    "            logger=logger,\n",
    "            device=device,\n",
    "            t_ops_config_path=t_ops_config_path\n",
    "        )\n",
    "vae.to(device)\n",
    "vae.eval()\n",
    "vae.enable_tiling()\n",
    "model=vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.tile_overlap_factor=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyvideo.vae.adaptive_temporal_tiling import AdaptiveTemporalTiling\n",
    "\n",
    "adaptor = AdaptiveTemporalTiling(\n",
    "        vae_ckpt_path=\"ckpts/hunyuan-video-t2v-720p/vae\",\n",
    "        device=device,\n",
    "        vae_precision=\"fp16\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset_processor.dataset_loader import VideoTensorDataset\n",
    "\n",
    "tensor_dir=\"video_data/processed_720p_tensors\"\n",
    "dataset = VideoTensorDataset(tensor_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "video_tensor, file_name=dataset[0]\n",
    "video_tensor=F.interpolate(video_tensor, size=(720//2,1280//2))\n",
    "video_tensor=video_tensor.unsqueeze(0)[:,:,:200]\n",
    "video_tensor = video_tensor.to(device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(video_tensor.shape,file_name)\n",
    "# with torch.no_grad():\n",
    "#     # Encode and decode video\n",
    "#     reconstructed_video = model(\n",
    "#         video_tensor,\n",
    "#         return_dict=False,\n",
    "#         return_posterior=True,\n",
    "#         sample_posterior=False\n",
    "#     )[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    # =========== Encode阶段 ===========\n",
    "    posterior_out = model.temporal_tiled_encode(\n",
    "        x=video_tensor,\n",
    "        adaptor=adaptor,  # 传入我们自适应类\n",
    "        return_dict=True\n",
    "    )\n",
    "    # posterior_out 是 AutoencoderKLOutput\n",
    "    posterior = posterior_out.latent_dist  # 里面包含 mean/var\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "# =========== Decode阶段 ===========\n",
    "    reconstructed_video = model.temporal_tiled_decode(\n",
    "        z=posterior.mode(),  # 或 sample() 取随机\n",
    "        adaptor=adaptor,     # 同样传入\n",
    "        return_dict=True\n",
    "    ).sample\n",
    "print(video_tensor.shape)\n",
    "print(reconstructed_video.shape)\n",
    "show_video(reconstructed_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_video(reconstructed_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HunyuanVideo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
