程序入口是:infer.py:#!/usr/bin/env python3
# infer.py

import argparse
import os
import torch
from torch.utils.data import DataLoader
from loguru import logger

from dataset_processor.dataset_loader import VideoTensorDataset
from hyvideo.vae.autoencoder_kl_causal_3d import AutoencoderKLCausal3D
from hyvideo.vae import load_vae
from hyvideo.utils.file_utils import save_videos_grid


def save_model_architecture_to_file(module, file_path: str):
"""将模型架构保存到文件，示例功能不变，可选。"""
with open(file_path, "w") as f:
def write_full_model(module, indent: int = 0):
prefix = " " * indent
f.write(f"{prefix}{module.__class__.__name__}:\n")
for name, sub_module in module.named_children():
f.write(f"{prefix} ({name}): {sub_module}\n")
write_full_model(sub_module, indent + 4)
write_full_model(module)


def infer_vae(model: AutoencoderKLCausal3D,
dataloader: DataLoader,
device: str,
output_dir: str,
max_files: int = None,
mp4: bool = False):
"""
Perform inference using the VAE model on video tensors.
"""
model.to(device)
model.eval()

os.makedirs(output_dir, exist_ok=True)

for batch_idx, (video_tensor, file_name) in enumerate(dataloader):
if max_files is not None and batch_idx >= max_files:
break # Stop processing after reaching the max number of files

# 去掉 .pt 后缀
file_name = file_name[0].replace(".pt", "")

# Move to device
video_tensor = video_tensor.to(device, dtype=torch.float16)
logger.info(f"Processing {file_name}, video shape: {video_tensor.shape}")

with torch.no_grad():
# Encode and decode video
reconstructed_video = model(
video_tensor,
return_dict=False,
return_posterior=True,
sample_posterior=False
)[0]

# Save the reconstructed video in .pt format
reconstructed_video = reconstructed_video.cpu().float()
output_path = os.path.join(output_dir, f"{file_name}.pt")
torch.save(reconstructed_video, output_path)
logger.info(f"Saved reconstructed video to {output_path}, shape: {reconstructed_video.shape}")

# Optionally save mp4
if mp4:
save_path = os.path.join(output_dir, f"{file_name}.mp4")
save_videos_grid(reconstructed_video, save_path, fps=24, rescale=True)
logger.info(f'Sample saved to: {save_path}')


def parse_args():
parser = argparse.ArgumentParser(description="VAE Inference script for video tensors.")
parser.add_argument("--tensor-dir", type=str, required=True,
help="Directory containing input .pt video tensors.")
parser.add_argument("--output-dir", type=str, required=True,
help="Directory to save the reconstructed videos.")
parser.add_argument("--vae-path", type=str, default="ckpts/hunyuan-video-t2v-720p/vae",
help="Path to VAE checkpoint directory (contains pytorch_model.pt).")
parser.add_argument("--config-json", type=str, default="t_ops_config.json",
help="Path to the T-ops config JSON file.")
parser.add_argument("--max-files", type=int, default=None,
help="Max number of input files to process (for quick testing).")
parser.add_argument("--mp4", action="store_true",
help="If set, also save outputs as .mp4 videos.")
parser.add_argument("--batch-size", type=int, default=1,
help="Batch size for data loader.")
parser.add_argument("--num-workers", type=int, default=4,
help="Number of workers for data loader.")
return parser.parse_args()


def main():
args = parse_args()
logger.info(f"Running inference with args: {args}")

device = "cuda" if torch.cuda.is_available() else "cpu"

# 加载 VAE 模型
logger.info("Loading VAE...")
vae, _, s_ratio, t_ratio = load_vae(
vae_type="884-16c-hy", # 这里和你们项目里保持一致
vae_precision="fp16",
logger=logger,
vae_path=args.vae_path,
device=device,
t_ops_config_path=args.config_json,
test=True
)
logger.info("VAE loaded.")

# 如果你想启用 tiling，可在此处启用
vae.enable_tiling()

# 加载数据集
dataset = VideoTensorDataset(args.tensor_dir)
dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)

# 运行推理
infer_vae(vae, dataloader, device, args.output_dir, max_files=args.max_files, mp4=args.mp4)


if __name__ == "__main__":
main()
__init__.py内容是:from pathlib import Path

import torch

from .autoencoder_kl_causal_3d import AutoencoderKLCausal3D
from ..constants import VAE_PATH, PRECISION_TO_TYPE

from pathlib import Path
from loguru import logger
import torch
import json
from .autoencoder_kl_causal_3d import AutoencoderKLCausal3D
from ..constants import VAE_PATH, PRECISION_TO_TYPE

def _apply_t_ops_config_to_vae(vae: AutoencoderKLCausal3D, t_ops_config: dict):
"""
将 t_ops_config 注入到 vae.encoder.down_blocks / vae.encoder.mid_block / vae.decoder.up_blocks / vae.decoder.mid_block.
"""
# -------------- 1) encoder --------------
enc_cfg = t_ops_config.get("encoder", {})

# (1a) 遍历 down_blocks
down_blocks_cfg = enc_cfg.get("down_blocks", [])
for block_cfg in down_blocks_cfg:
idx = block_cfg["block_index"]
if 0 <= idx < len(vae.encoder.down_blocks):
down_block = vae.encoder.down_blocks[idx]
if hasattr(down_block, "apply_t_ops_config"):
down_block.apply_t_ops_config(block_cfg)
else:
print(f"[Warning] down_block at index {idx} lacks apply_t_ops_config().")
else:
print(f"[Warning] down_block index {idx} out of range of encoder.down_blocks.")

# (1b) encoder.mid_block
enc_mid_cfg = enc_cfg.get("mid_block", {}) # <=== 新增
if hasattr(vae.encoder, "mid_block") and hasattr(vae.encoder.mid_block, "apply_t_ops_config_midblock"):
vae.encoder.mid_block.apply_t_ops_config_midblock(enc_mid_cfg)
else:
print("[Warning] encoder.mid_block not found or has no apply_t_ops_config_midblock method.")

# -------------- 2) decoder --------------
dec_cfg = t_ops_config.get("decoder", {})

# (2a) 遍历 up_blocks
up_blocks_cfg = dec_cfg.get("up_blocks", [])
for block_cfg in up_blocks_cfg:
idx = block_cfg["block_index"]
if 0 <= idx < len(vae.decoder.up_blocks):
up_block = vae.decoder.up_blocks[idx]
if hasattr(up_block, "apply_t_ops_config"):
up_block.apply_t_ops_config(block_cfg)
else:
print(f"[Warning] up_block at index {idx} lacks apply_t_ops_config().")
else:
print(f"[Warning] up_block index {idx} out of range of decoder.up_blocks.")

# (2b) decoder.mid_block
dec_mid_cfg = dec_cfg.get("mid_block", {}) # <=== 新增
if hasattr(vae.decoder, "mid_block") and hasattr(vae.decoder.mid_block, "apply_t_ops_config_midblock"):
vae.decoder.mid_block.apply_t_ops_config_midblock(dec_mid_cfg)
else:
print("[Warning] decoder.mid_block not found or has no apply_t_ops_config_midblock method.")


def load_t_ops_config(json_path: str) -> dict:
with open(json_path, "r") as f:
return json.load(f)

def load_vae(
vae_type: str="884-16c-hy",
vae_precision: str=None,
sample_size: tuple=None,
vae_path: str=None,
logger=None,
device=None,
t_ops_config_path: str = None,
test: bool = False,
):
"""
Load the 3D VAE model.
"""
if vae_path is None:
vae_path = VAE_PATH[vae_type]

if logger is not None:
logger.info(f"Loading 3D VAE model ({vae_type}) from: {vae_path}")
config = AutoencoderKLCausal3D.load_config(vae_path)
if sample_size:
vae = AutoencoderKLCausal3D.from_config(config, sample_size=sample_size)
else:
vae = AutoencoderKLCausal3D.from_config(config)

vae_ckpt = Path(vae_path) / "pytorch_model.pt"
assert vae_ckpt.exists(), f"VAE checkpoint not found: {vae_ckpt}"

ckpt = torch.load(vae_ckpt, map_location=vae.device, weights_only=False)
if "state_dict" in ckpt:
ckpt = ckpt["state_dict"]
if any(k.startswith("vae.") for k in ckpt.keys()):
ckpt = {k.replace("vae.", ""): v for k, v in ckpt.items() if k.startswith("vae.")}
vae.load_state_dict(ckpt)

spatial_compression_ratio = vae.config.spatial_compression_ratio
time_compression_ratio = vae.config.time_compression_ratio

if vae_precision is not None:
vae = vae.to(dtype=PRECISION_TO_TYPE[vae_precision])

vae.requires_grad_(False)

if logger is not None:
logger.info(f"VAE to dtype: {vae.dtype}")

if device is not None:
vae = vae.to(device)

vae.eval()

# ============ 如果传入了 t_ops_config_path，并且 test=True，就调用 _apply_t_ops_config_to_vae ============
if t_ops_config_path is not None and test:
my_t_ops_config = load_t_ops_config(t_ops_config_path)
if logger is not None:
logger.info("Applying T-pool/pad configs to the loaded VAE.")
_apply_t_ops_config_to_vae(vae, my_t_ops_config)

return vae, vae_path, spatial_compression_ratio, time_compression_ratio
autoencoder_ki_causal_3d.py内容是:# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
#
# Modified from diffusers==0.29.2
#
# ==============================================================================
from typing import Dict, Optional, Tuple, Union
from dataclasses import dataclass
from loguru import logger
import torch
import torch.nn as nn

from diffusers.configuration_utils import ConfigMixin, register_to_config

try:
# This diffusers is modified and packed in the mirror.
from diffusers.loaders import FromOriginalVAEMixin
except ImportError:
# Use this to be compatible with the original diffusers.
from diffusers.loaders.single_file_model import FromOriginalModelMixin as FromOriginalVAEMixin
from diffusers.utils.accelerate_utils import apply_forward_hook
from diffusers.models.attention_processor import (
ADDED_KV_ATTENTION_PROCESSORS,
CROSS_ATTENTION_PROCESSORS,
Attention,
AttentionProcessor,
AttnAddedKVProcessor,
AttnProcessor,
)
from diffusers.models.modeling_outputs import AutoencoderKLOutput
from diffusers.models.modeling_utils import ModelMixin
from .vae import DecoderCausal3D, BaseOutput, DecoderOutput, DiagonalGaussianDistribution, EncoderCausal3D


@dataclass
class DecoderOutput2(BaseOutput):
sample: torch.FloatTensor
posterior: Optional[DiagonalGaussianDistribution] = None


class AutoencoderKLCausal3D(ModelMixin, ConfigMixin, FromOriginalVAEMixin):
r"""
A VAE model with KL loss for encoding images/videos into latents and decoding latent representations into images/videos.

This model inherits from [ModelMixin]. Check the superclass documentation for it's generic methods implemented
for all models (such as downloading or saving).
"""

_supports_gradient_checkpointing = True

@register_to_config
def __init__(
self,
in_channels: int = 3,
out_channels: int = 3,
down_block_types: Tuple[str] = ("DownEncoderBlockCausal3D",),
up_block_types: Tuple[str] = ("UpDecoderBlockCausal3D",),
block_out_channels: Tuple[int] = (64,),
layers_per_block: int = 1,
act_fn: str = "silu",
latent_channels: int = 4,
norm_num_groups: int = 32,
sample_size: int = 32,
sample_tsize: int = 64,
scaling_factor: float = 0.18215,
force_upcast: float = True,
spatial_compression_ratio: int = 8,
time_compression_ratio: int = 4,
mid_block_add_attention: bool = True,
):
super().__init__()

self.time_compression_ratio = time_compression_ratio

self.encoder = EncoderCausal3D(
in_channels=in_channels,
out_channels=latent_channels,
down_block_types=down_block_types,
block_out_channels=block_out_channels,
layers_per_block=layers_per_block,
act_fn=act_fn,
norm_num_groups=norm_num_groups,
double_z=True, #如果为True，将z的维度扩大一倍,2*latent_channels
time_compression_ratio=time_compression_ratio,
spatial_compression_ratio=spatial_compression_ratio,
mid_block_add_attention=mid_block_add_attention,
)

self.decoder = DecoderCausal3D(
in_channels=latent_channels,
out_channels=out_channels,
up_block_types=up_block_types,
block_out_channels=block_out_channels,
layers_per_block=layers_per_block,
norm_num_groups=norm_num_groups,
act_fn=act_fn,
time_compression_ratio=time_compression_ratio,
spatial_compression_ratio=spatial_compression_ratio,
mid_block_add_attention=mid_block_add_attention,
)
#只是简单的线性变化
self.quant_conv = nn.Conv3d(2 * latent_channels, 2 * latent_channels, kernel_size=1)
self.post_quant_conv = nn.Conv3d(latent_channels, latent_channels, kernel_size=1)

self.use_slicing = False
self.use_spatial_tiling = False
self.use_temporal_tiling = False

# only relevant if vae tiling is enabled
self.tile_sample_min_tsize = sample_tsize
self.tile_latent_min_tsize = sample_tsize // time_compression_ratio

self.tile_sample_min_size = self.config.sample_size
sample_size = (
self.config.sample_size[0]
if isinstance(self.config.sample_size, (list, tuple))
else self.config.sample_size
)
self.tile_latent_min_size = int(sample_size / (2 ** (len(self.config.block_out_channels) - 1)))#是指tile经过encode以后的大小
self.tile_overlap_factor = 0.25

def _set_gradient_checkpointing(self, module, value=False):
if isinstance(module, (EncoderCausal3D, DecoderCausal3D)):
module.gradient_checkpointing = value

def enable_temporal_tiling(self, use_tiling: bool = True):
self.use_temporal_tiling = use_tiling

def disable_temporal_tiling(self):
self.enable_temporal_tiling(False)

def enable_spatial_tiling(self, use_tiling: bool = True):
self.use_spatial_tiling = use_tiling

def disable_spatial_tiling(self):
self.enable_spatial_tiling(False)

def enable_tiling(self, use_tiling: bool = True):
r"""
Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to
compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow
processing larger videos.
"""
self.enable_spatial_tiling(use_tiling)
self.enable_temporal_tiling(use_tiling)

def disable_tiling(self):
r"""
Disable tiled VAE decoding. If enable_tiling was previously enabled, this method will go back to computing
decoding in one step.
"""
self.disable_spatial_tiling()
self.disable_temporal_tiling()

def enable_slicing(self):
r"""
Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to
compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.
"""
self.use_slicing = True

def disable_slicing(self):
r"""
Disable sliced VAE decoding. If enable_slicing was previously enabled, this method will go back to computing
decoding in one step.
"""
self.use_slicing = False

@property
# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.attn_processors
def attn_processors(self) -> Dict[str, AttentionProcessor]:
r"""
Returns:
dict of attention processors: A dictionary containing all attention processors used in the model with
indexed by its weight name.
"""
# set recursively
processors = {}

def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
if hasattr(module, "get_processor"):
processors[f"{name}.processor"] = module.get_processor(return_deprecated_lora=True)

for sub_name, child in module.named_children():
fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)

return processors

for name, module in self.named_children():
fn_recursive_add_processors(name, module, processors)

return processors

# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.set_attn_processor
def set_attn_processor(
self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]], _remove_lora=False
):
r"""
Sets the attention processor to use to compute attention.

Parameters:
processor (dict of AttentionProcessor or only AttentionProcessor):
The instantiated processor class or a dictionary of processor classes that will be set as the processor
for **all** Attention layers.

If processor is a dict, the key needs to define the path to the corresponding cross attention
processor. This is strongly recommended when setting trainable attention processors.

"""
count = len(self.attn_processors.keys())

if isinstance(processor, dict) and len(processor) != count:
raise ValueError(
f"A dict of processors was passed, but the number of processors {len(processor)} does not match the"
f" number of attention layers: {count}. Please make sure to pass {count} processor classes."
)

def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
if hasattr(module, "set_processor"):
if not isinstance(processor, dict):
module.set_processor(processor, _remove_lora=_remove_lora)
else:
module.set_processor(processor.pop(f"{name}.processor"), _remove_lora=_remove_lora)

for sub_name, child in module.named_children():
fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)

for name, module in self.named_children():
fn_recursive_attn_processor(name, module, processor)

# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.set_default_attn_processor
def set_default_attn_processor(self):
"""
Disables custom attention processors and sets the default attention implementation.
"""
if all(proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
processor = AttnAddedKVProcessor()
elif all(proc.__class__ in CROSS_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
processor = AttnProcessor()
else:
raise ValueError(
f"Cannot call set_default_attn_processor when attention processors are of type {next(iter(self.attn_processors.values()))}"
)

self.set_attn_processor(processor, _remove_lora=True)

@apply_forward_hook
def encode(
self, x: torch.FloatTensor, return_dict: bool = True
) -> Union[AutoencoderKLOutput, Tuple[DiagonalGaussianDistribution]]:
"""
Encode a batch of images/videos into latents.

Args:
x (torch.FloatTensor): Input batch of images/videos.
return_dict (bool, *optional*, defaults to True):
Whether to return a [~models.autoencoder_kl.AutoencoderKLOutput] instead of a plain tuple.

Returns:
The latent representations of the encoded images/videos. If return_dict is True, a
[~models.autoencoder_kl.AutoencoderKLOutput] is returned, otherwise a plain tuple is returned.
"""
assert len(x.shape) == 5, "The input tensor should have 5 dimensions."
# (B, C, T, H ,W)
print(f"王思远x shape: {x.shape}, {self.tile_sample_min_tsize}")

if self.use_temporal_tiling and x.shape[2] > self.tile_sample_min_tsize: #时间压缩中可以包含空间压缩
print(f"时空压缩")
return self.temporal_tiled_encode(x, return_dict=return_dict) #只返回分布 不返回数值

if self.use_spatial_tiling and (x.shape[-1] > self.tile_sample_min_size or x.shape[-2] > self.tile_sample_min_size):
print(f"空间压缩")
return self.spatial_tiled_encode(x, return_dict=return_dict) #只返回分布 不返回数值

if self.use_slicing and x.shape[0] > 1:#为了节省内存，将输入切片
encoded_slices = [self.encoder(x_slice) for x_slice in x.split(1)]
h = torch.cat(encoded_slices)
else:
h = self.encoder(x)

moments = self.quant_conv(h)
posterior = DiagonalGaussianDistribution(moments)

if not return_dict:
return (posterior,)

return AutoencoderKLOutput(latent_dist=posterior)

def _decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:
assert len(z.shape) == 5, "The input tensor should have 5 dimensions."

if self.use_temporal_tiling and z.shape[2] > self.tile_latent_min_tsize:
return self.temporal_tiled_decode(z, return_dict=return_dict)

if self.use_spatial_tiling and (z.shape[-1] > self.tile_latent_min_size or z.shape[-2] > self.tile_latent_min_size):
return self.spatial_tiled_decode(z, return_dict=return_dict)

z = self.post_quant_conv(z) #post_quant_conv是一个1x1的卷积,直线线性变换
dec = self.decoder(z) #decoder是一个解码器，将z解码成图片 此时channel是3

if not return_dict:
return (dec,)

return DecoderOutput(sample=dec)
#实际应用的decode
@apply_forward_hook
def decode(
self, z: torch.FloatTensor, return_dict: bool = True, generator=None
) -> Union[DecoderOutput, torch.FloatTensor]:
"""
Decode a batch of images/videos.

Args:
z (torch.FloatTensor): Input batch of latent vectors.
return_dict (bool, *optional*, defaults to True):
Whether to return a [~models.vae.DecoderOutput] instead of a plain tuple.

Returns:
[~models.vae.DecoderOutput] or tuple:
If return_dict is True, a [~models.vae.DecoderOutput] is returned, otherwise a plain tuple is
returned.

"""
if self.use_slicing and z.shape[0] > 1:#这段是为了节省内存，将输入切片
decoded_slices = [self._decode(z_slice).sample for z_slice in z.split(1)]
decoded = torch.cat(decoded_slices)
else:
decoded = self._decode(z).sample

if not return_dict:
return (decoded,)

return DecoderOutput(sample=decoded)
#上下混合 #B, C, T, H, W
def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:
blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)
for y in range(blend_extent):
b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, :, y, :] * (y / blend_extent)
return b
#左右混合
def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:
blend_extent = min(a.shape[-1], b.shape[-1], blend_extent)
for x in range(blend_extent):
b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, :, x] * (x / blend_extent)
return b

def blend_t(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:
blend_extent = min(a.shape[-3], b.shape[-3], blend_extent)
for x in range(blend_extent):
b[:, :, x, :, :] = a[:, :, -blend_extent + x, :, :] * (1 - x / blend_extent) + b[:, :, x, :, :] * (x / blend_extent)
return b

def spatial_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True, return_moments: bool = False) -> AutoencoderKLOutput:
r"""Encode a batch of images/videos using a tiled encoder.

When this option is enabled, the VAE will split the input tensor into tiles to compute encoding in several
steps. This is useful to keep memory use constant regardless of image/videos size. The end result of tiled encoding is
different from non-tiled encoding because each tile uses a different encoder. To avoid tiling artifacts, the
tiles overlap and are blended together to form a smooth output. You may still see tile-sized changes in the
output, but they should be much less noticeable.

Args:
x (torch.FloatTensor): Input batch of images/videos.
return_dict (bool, *optional*, defaults to True):
Whether or not to return a [~models.autoencoder_kl.AutoencoderKLOutput] instead of a plain tuple.

Returns:
[~models.autoencoder_kl.AutoencoderKLOutput] or tuple:
If return_dict is True, a [~models.autoencoder_kl.AutoencoderKLOutput] is returned, otherwise a plain
tuple is returned.
"""
#滑动步长
overlap_size = int(self.tile_sample_min_size * (1 - self.tile_overlap_factor)) #tile_sample_min_size = 256
#重叠区域
blend_extent = int(self.tile_latent_min_size * self.tile_overlap_factor)
#有效区域
row_limit = self.tile_latent_min_size - blend_extent
# Split video into tiles and encode them separately.
rows = []#B, C, T, H, W
for i in range(0, x.shape[-2], overlap_size):
row = []
for j in range(0, x.shape[-1], overlap_size):
tile = x[:, :, :, i: i + self.tile_sample_min_size, j: j + self.tile_sample_min_size]
tile = self.encoder(tile)
tile = self.quant_conv(tile)
row.append(tile)
rows.append(row)
result_rows = []
for i, row in enumerate(rows):
result_row = []
for j, tile in enumerate(row):
# blend the above tile and the left tile
# to the current tile and add the current tile to the result row
if i > 0:
#上面的tile和当前tile混合
tile = self.blend_v(rows[i - 1][j], tile, blend_extent)
if j > 0:
#左边的tile和当前tile混合
tile = self.blend_h(row[j - 1], tile, blend_extent)
result_row.append(tile[:, :, :, :row_limit, :row_limit])
result_rows.append(torch.cat(result_row, dim=-1))

moments = torch.cat(result_rows, dim=-2) #经过空间压缩后的小feature map
if return_moments:
return moments

posterior = DiagonalGaussianDistribution(moments)
if not return_dict:
return (posterior,)

return AutoencoderKLOutput(latent_dist=posterior)

def spatial_tiled_decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:
r"""
Decode a batch of images/videos using a tiled decoder.

Args:
z (torch.FloatTensor): Input batch of latent vectors.
return_dict (bool, *optional*, defaults to True):
Whether or not to return a [~models.vae.DecoderOutput] instead of a plain tuple.

Returns:
[~models.vae.DecoderOutput] or tuple:
If return_dict is True, a [~models.vae.DecoderOutput] is returned, otherwise a plain tuple is
returned.
"""
overlap_size = int(self.tile_latent_min_size * (1 - self.tile_overlap_factor))#滑动步长
blend_extent = int(self.tile_sample_min_size * self.tile_overlap_factor)#重叠区域
row_limit = self.tile_sample_min_size - blend_extent#有效区域

# Split z into overlapping tiles and decode them separately.
# The tiles have an overlap to avoid seams between tiles.
rows = []
for i in range(0, z.shape[-2], overlap_size):#z的维度是B, C, T, H, W
row = []
for j in range(0, z.shape[-1], overlap_size):
tile = z[:, :, :, i: i + self.tile_latent_min_size, j: j + self.tile_latent_min_size]
tile = self.post_quant_conv(tile)
decoded = self.decoder(tile)
row.append(decoded)
rows.append(row)
result_rows = []
for i, row in enumerate(rows):
result_row = []
for j, tile in enumerate(row):
# blend the above tile and the left tile
# to the current tile and add the current tile to the result row
# 以下是要解决原始大小的feature map重叠的问题
if i > 0:
tile = self.blend_v(rows[i - 1][j], tile, blend_extent)
if j > 0:
tile = self.blend_h(row[j - 1], tile, blend_extent)
result_row.append(tile[:, :, :, :row_limit, :row_limit])
result_rows.append(torch.cat(result_row, dim=-1))

dec = torch.cat(result_rows, dim=-2)
if not return_dict:
return (dec,)

return DecoderOutput(sample=dec)

def temporal_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True) -> AutoencoderKLOutput:

B, C, T, H, W = x.shape
#滑动距离
overlap_size = int(self.tile_sample_min_tsize * (1 - self.tile_overlap_factor))
#重叠距离 是在latent维度计算的
blend_extent = int(self.tile_latent_min_tsize * self.tile_overlap_factor)
#有效距离
t_limit = self.tile_latent_min_tsize - blend_extent

# Split the video into tiles and encode them separately.
row = []
for i in range(0, T, overlap_size):#i + self.tile_sample_min_tsize + 1 里的+1是因为tile的最后一个时间步是下一个tile的第一个时间步
tile = x[:, :, i: i + self.tile_sample_min_tsize + 1, :, :] #空间快是256*256,时间快是64
logger.info(f"王思远tile shape: {tile.shape}")
if self.use_spatial_tiling and (tile.shape[-1] > self.tile_sample_min_size or tile.shape[-2] > self.tile_sample_min_size):
tile = self.spatial_tiled_encode(tile, return_moments=True)#只返回moments，不返回分布
else:
tile = self.encoder(tile) #传入VAE文件中的encoder，进行了一系列下采样，以及channel维度扩容
tile = self.quant_conv(tile)
if i > 0:
logger.info(f"王思远tile shape: {tile.shape}")
tile = tile[:, :, 1:, :, :] #这里面的1是因为tile的最后一个时间步是下一个tile的第一个时间步
row.append(tile)
result_row = []
for i, tile in enumerate(row):
if i > 0:
tile = self.blend_t(row[i - 1], tile, blend_extent)
result_row.append(tile[:, :, :t_limit, :, :])
else:
result_row.append(tile[:, :, :t_limit + 1, :, :])

moments = torch.cat(result_row, dim=2)

posterior = DiagonalGaussianDistribution(moments)

if not return_dict:
return (posterior,)

return AutoencoderKLOutput(latent_dist=posterior)

def temporal_tiled_decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:
# Split z into overlapping tiles and decode them separately.

B, C, T, H, W = z.shape
overlap_size = int(self.tile_latent_min_tsize * (1 - self.tile_overlap_factor))
blend_extent = int(self.tile_sample_min_tsize * self.tile_overlap_factor)
t_limit = self.tile_sample_min_tsize - blend_extent

row = []
for i in range(0, T, overlap_size):
tile = z[:, :, i: i + self.tile_latent_min_tsize + 1, :, :]
if self.use_spatial_tiling and (tile.shape[-1] > self.tile_latent_min_size or tile.shape[-2] > self.tile_latent_min_size):
decoded = self.spatial_tiled_decode(tile, return_dict=True).sample
else:
tile = self.post_quant_conv(tile)
decoded = self.decoder(tile)
if i > 0:
decoded = decoded[:, :, 1:, :, :]
row.append(decoded)
result_row = []
for i, tile in enumerate(row):
if i > 0:
tile = self.blend_t(row[i - 1], tile, blend_extent)
result_row.append(tile[:, :, :t_limit, :, :])
else:
result_row.append(tile[:, :, :t_limit + 1, :, :])

dec = torch.cat(result_row, dim=2)
if not return_dict:
return (dec,)

return DecoderOutput(sample=dec)

def forward(
self,
sample: torch.FloatTensor,
sample_posterior: bool = False,
return_dict: bool = True,
return_posterior: bool = False,
generator: Optional[torch.Generator] = None,
) -> Union[DecoderOutput2, torch.FloatTensor]:
r"""
Args:
sample (torch.FloatTensor): Input sample.
sample_posterior (bool, *optional*, defaults to False):
Whether to sample from the posterior.
return_dict (bool, *optional*, defaults to True):
Whether or not to return a [DecoderOutput] instead of a plain tuple.
"""
x = sample
# latent_dist=posterior是一个分布
# encode输出的是一个分布,其中均值维度是B, C, T, H, W,其中C是latent_channels,方差也是这个维度.
# 这就代表vae学习到的是latent_channels纬度的高斯分布
posterior = self.encode(x).latent_dist
if sample_posterior:
z = posterior.sample(generator=generator) #z的形状是B, C, T, H, W,其中C具体数值是latent_channels=4
else:
z = posterior.mode()
dec = self.decode(z).sample #这时候channel是3

if not return_dict:
if return_posterior:
return (dec, posterior)
else:
return (dec,)
if return_posterior:
return DecoderOutput2(sample=dec, posterior=posterior)
else:
return DecoderOutput2(sample=dec)

# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections
def fuse_qkv_projections(self):
"""
Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query,
key, value) are fused. For cross-attention modules, key and value projection matrices are fused.

<Tip warning={true}>

This API is 🧪 experimental.

</Tip>
"""
self.original_attn_processors = None

for _, attn_processor in self.attn_processors.items():
if "Added" in str(attn_processor.__class__.__name__):
raise ValueError("fuse_qkv_projections() is not supported for models having added KV projections.")

self.original_attn_processors = self.attn_processors

for module in self.modules():
if isinstance(module, Attention):
module.fuse_projections(fuse=True)

# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections
def unfuse_qkv_projections(self):
"""Disables the fused QKV projection if enabled.

<Tip warning={true}>

This API is 🧪 experimental.

</Tip>

"""
if self.original_attn_processors is not None:
self.set_attn_processor(self.original_attn_processors)
unet_causal_3d_blocks.py内容是:# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
#
# Modified from diffusers==0.29.2
#
# ==============================================================================

from typing import Optional, Tuple, Union

import torch
import torch.nn.functional as F
from torch import nn
from einops import rearrange

from diffusers.utils import logging
from diffusers.models.activations import get_activation
from diffusers.models.attention_processor import SpatialNorm
from diffusers.models.attention_processor import Attention
from diffusers.models.normalization import AdaGroupNorm
from diffusers.models.normalization import RMSNorm
from loguru import logger as _logger
import sys

logger_ = logging.get_logger(__name__) # pylint: disable=invalid-name

def prepare_causal_attention_mask(n_frame: int, n_hw: int, dtype, device, batch_size: int = None):
seq_len = n_frame * n_hw
mask = torch.full((seq_len, seq_len), float("-inf"), dtype=dtype, device=device)
for i in range(seq_len):
i_frame = i // n_hw
mask[i, : (i_frame + 1) * n_hw] = 0
if batch_size is not None:
mask = mask.unsqueeze(0).expand(batch_size, -1, -1)
return mask


class CausalConv3d(nn.Module):
"""
Implements a causal 3D convolution layer where each position only depends on previous timesteps and current spatial locations.
This maintains temporal causality in video generation tasks.
"""

def __init__(
self,
chan_in,
chan_out,
kernel_size: Union[int, Tuple[int, int, int]],
stride: Union[int, Tuple[int, int, int]] = 1,
dilation: Union[int, Tuple[int, int, int]] = 1,#不开启膨胀卷积
pad_mode='replicate',
**kwargs
):
super().__init__()

self.pad_mode = pad_mode
padding = (kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size - 1, 0) # W, H, T
self.time_causal_padding = padding

self.conv = nn.Conv3d(chan_in, chan_out, kernel_size, stride=stride, dilation=dilation, **kwargs)

def forward(self, x):
x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)
return self.conv(x)


class UpsampleCausal3D(nn.Module):
"""
A 3D upsampling layer with an optional convolution.
"""

def __init__(
self,
channels: int,
use_conv: bool = False,
use_conv_transpose: bool = False,
out_channels: Optional[int] = None,
name: str = "conv",
kernel_size: Optional[int] = None,
padding=1,
norm_type=None,
eps=None,
elementwise_affine=None,
bias=True,
interpolate=True,
upsample_factor=(2, 2, 2),
):
super().__init__()
self.channels = channels
self.out_channels = out_channels or channels
self.use_conv = use_conv
self.use_conv_transpose = use_conv_transpose
self.name = name
self.interpolate = interpolate
self.upsample_factor = upsample_factor

if norm_type == "ln_norm":
self.norm = nn.LayerNorm(channels, eps, elementwise_affine)
elif norm_type == "rms_norm":
self.norm = RMSNorm(channels, eps, elementwise_affine)
elif norm_type is None:
self.norm = None
else:
raise ValueError(f"unknown norm_type: {norm_type}")

conv = None
if use_conv_transpose:
raise NotImplementedError
elif use_conv:
if kernel_size is None:
kernel_size = 3
conv = CausalConv3d(self.channels, self.out_channels, kernel_size=kernel_size, bias=bias)

if name == "conv":
self.conv = conv
else:
self.Conv2d_0 = conv

def forward(
self,
hidden_states: torch.FloatTensor,
output_size: Optional[int] = None,
scale: float = 1.0,
) -> torch.FloatTensor:
assert hidden_states.shape[1] == self.channels

if self.norm is not None:
raise NotImplementedError

if self.use_conv_transpose:
return self.conv(hidden_states)

# Cast to float32 to as 'upsample_nearest2d_out_frame' op does not support bfloat16
dtype = hidden_states.dtype
if dtype == torch.bfloat16:
hidden_states = hidden_states.to(torch.float32)

# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984
if hidden_states.shape[0] >= 64:
hidden_states = hidden_states.contiguous()

# if output_size is passed we force the interpolation output
# size and do not make use of scale_factor=2
if self.interpolate:
B, C, T, H, W = hidden_states.shape
first_h, other_h = hidden_states.split((1, T - 1), dim=2)
if output_size is None:
if T > 1:
other_h = F.interpolate(other_h, scale_factor=self.upsample_factor, mode="nearest")

first_h = first_h.squeeze(2)
first_h = F.interpolate(first_h, scale_factor=self.upsample_factor[1:], mode="nearest")
first_h = first_h.unsqueeze(2)
else:
raise NotImplementedError

if T > 1:
hidden_states = torch.cat((first_h, other_h), dim=2)
else:
hidden_states = first_h

# If the input is bfloat16, we cast back to bfloat16
if dtype == torch.bfloat16:
hidden_states = hidden_states.to(dtype)

if self.use_conv:
if self.name == "conv":
hidden_states = self.conv(hidden_states)
else:
hidden_states = self.Conv2d_0(hidden_states)

return hidden_states


class DownsampleCausal3D(nn.Module):
"""
A 3D downsampling layer with an optional convolution.
"""

def __init__(
self,
channels: int,
use_conv: bool = False,
out_channels: Optional[int] = None,
padding: int = 1,
name: str = "conv",
kernel_size=3,
norm_type=None,
eps=None,
elementwise_affine=None,
bias=True,
stride=2,#1pad,2stride,3kernel可以实现减半
):
super().__init__()
self.channels = channels
self.out_channels = out_channels or channels
self.use_conv = use_conv
self.padding = padding #没用上这个参数
stride = stride
self.name = name
#都是在通道维度
if norm_type == "ln_norm":
self.norm = nn.LayerNorm(channels, eps, elementwise_affine)
elif norm_type == "rms_norm":
self.norm = RMSNorm(channels, eps, elementwise_affine)
elif norm_type is None:
self.norm = None
else:
raise ValueError(f"unknown norm_type: {norm_type}")

if use_conv:
conv = CausalConv3d(
self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, bias=bias
)
else:
raise NotImplementedError

if name == "conv":
self.Conv2d_0 = conv
self.conv = conv
elif name == "Conv2d_0":
self.conv = conv
else:
self.conv = conv

def forward(self, hidden_states: torch.FloatTensor, scale: float = 1.0) -> torch.FloatTensor:
assert hidden_states.shape[1] == self.channels #[B, C, H, W]

if self.norm is not None:
hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)

assert hidden_states.shape[1] == self.channels

hidden_states = self.conv(hidden_states)

return hidden_states


class ResnetBlockCausal3D(nn.Module):
r"""
A Resnet block.
"""

def __init__(
self,
*,
in_channels: int,
out_channels: Optional[int] = None,
conv_shortcut: bool = False,
dropout: float = 0.0,
temb_channels: int = 512,
groups: int = 32,
groups_out: Optional[int] = None,
pre_norm: bool = True,
eps: float = 1e-6,
non_linearity: str = "swish",
skip_time_act: bool = False,
# default, scale_shift, ada_group, spatial
time_embedding_norm: str = "default",
kernel: Optional[torch.FloatTensor] = None,
output_scale_factor: float = 1.0,
use_in_shortcut: Optional[bool] = None,
up: bool = False,
down: bool = False,
conv_shortcut_bias: bool = True,
conv_3d_out_channels: Optional[int] = None,
):
super().__init__()
self.pre_norm = pre_norm
self.pre_norm = True
self.in_channels = in_channels
out_channels = in_channels if out_channels is None else out_channels
self.out_channels = out_channels
self.use_conv_shortcut = conv_shortcut
self.up = up
self.down = down
self.output_scale_factor = output_scale_factor
self.time_embedding_norm = time_embedding_norm
self.skip_time_act = skip_time_act

linear_cls = nn.Linear

if groups_out is None:
groups_out = groups

if self.time_embedding_norm == "ada_group":
self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)
elif self.time_embedding_norm == "spatial":
self.norm1 = SpatialNorm(in_channels, temb_channels)
else:
self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)
#第一次卷积 T,H,W维度没有发生变化,仅channel有可能变化
self.conv1 = CausalConv3d(in_channels, out_channels, kernel_size=3, stride=1)

if temb_channels is not None:
if self.time_embedding_norm == "default":
self.time_emb_proj = linear_cls(temb_channels, out_channels)
elif self.time_embedding_norm == "scale_shift":
self.time_emb_proj = linear_cls(temb_channels, 2 * out_channels)
elif self.time_embedding_norm == "ada_group" or self.time_embedding_norm == "spatial":
self.time_emb_proj = None
else:
raise ValueError(f"Unknown time_embedding_norm : {self.time_embedding_norm} ")
else:
self.time_emb_proj = None

if self.time_embedding_norm == "ada_group":
self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)
elif self.time_embedding_norm == "spatial":
self.norm2 = SpatialNorm(out_channels, temb_channels)
else:
self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)

self.dropout = torch.nn.Dropout(dropout)
conv_3d_out_channels = conv_3d_out_channels or out_channels
#第二次卷积 H,W维度没有发生变化,仅channel有可能变化
self.conv2 = CausalConv3d(out_channels, conv_3d_out_channels, kernel_size=3, stride=1)

self.nonlinearity = get_activation(non_linearity)

self.upsample = self.downsample = None
if self.up:
self.upsample = UpsampleCausal3D(in_channels, use_conv=False)
elif self.down:
self.downsample = DownsampleCausal3D(in_channels, use_conv=False, name="op")

self.use_in_shortcut = self.in_channels != conv_3d_out_channels if use_in_shortcut is None else use_in_shortcut

self.conv_shortcut = None
if self.use_in_shortcut:
self.conv_shortcut = CausalConv3d(
in_channels,
conv_3d_out_channels,
kernel_size=1,
stride=1,
bias=conv_shortcut_bias,
)

def forward(
self,
input_tensor: torch.FloatTensor,
temb: torch.FloatTensor,
scale: float = 1.0,
) -> torch.FloatTensor:
hidden_states = input_tensor

if self.time_embedding_norm == "ada_group" or self.time_embedding_norm == "spatial":
hidden_states = self.norm1(hidden_states, temb)
else:
hidden_states = self.norm1(hidden_states)

hidden_states = self.nonlinearity(hidden_states)

if self.upsample is not None:
# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984
if hidden_states.shape[0] >= 64:
input_tensor = input_tensor.contiguous()
hidden_states = hidden_states.contiguous()
input_tensor = (
self.upsample(input_tensor, scale=scale)
)
hidden_states = (
self.upsample(hidden_states, scale=scale)
)
elif self.downsample is not None:
input_tensor = (
self.downsample(input_tensor, scale=scale)
)
hidden_states = (
self.downsample(hidden_states, scale=scale)
)

hidden_states = self.conv1(hidden_states)

if self.time_emb_proj is not None:
if not self.skip_time_act:
temb = self.nonlinearity(temb)
temb = (
self.time_emb_proj(temb, scale)[:, :, None, None]
)

if temb is not None and self.time_embedding_norm == "default":
hidden_states = hidden_states + temb

if self.time_embedding_norm == "ada_group" or self.time_embedding_norm == "spatial":
hidden_states = self.norm2(hidden_states, temb)
else:
hidden_states = self.norm2(hidden_states)

if temb is not None and self.time_embedding_norm == "scale_shift":
scale, shift = torch.chunk(temb, 2, dim=1)
hidden_states = hidden_states * (1 + scale) + shift

hidden_states = self.nonlinearity(hidden_states)

hidden_states = self.dropout(hidden_states)
hidden_states = self.conv2(hidden_states)

if self.conv_shortcut is not None:
input_tensor = (
self.conv_shortcut(input_tensor)
)
#残差
output_tensor = (input_tensor + hidden_states) / self.output_scale_factor

return output_tensor

def get_down_block3d(
down_block_type: str,
num_layers: int,
in_channels: int,
out_channels: int,
temb_channels: int,
add_downsample: bool,
downsample_stride: int,
resnet_eps: float,
resnet_act_fn: str,
transformer_layers_per_block: int = 1,
num_attention_heads: Optional[int] = None,
resnet_groups: Optional[int] = None,
cross_attention_dim: Optional[int] = None,
downsample_padding: Optional[int] = None,
dual_cross_attention: bool = False,
use_linear_projection: bool = False,
only_cross_attention: bool = False,
upcast_attention: bool = False,
resnet_time_scale_shift: str = "default",
attention_type: str = "default",
resnet_skip_time_act: bool = False,
resnet_out_scale_factor: float = 1.0,
cross_attention_norm: Optional[str] = None,
attention_head_dim: Optional[int] = None,
downsample_type: Optional[str] = None,
dropout: float = 0.0,
):
# If attn head dim is not defined, we default it to the number of heads
if attention_head_dim is None:
logger_.warning(
f"It is recommended to provide attention_head_dim when calling get_down_block. Defaulting attention_head_dim to {num_attention_heads}."
)
attention_head_dim = num_attention_heads

down_block_type = down_block_type[7:] if down_block_type.startswith("UNetRes") else down_block_type
if down_block_type == "DownEncoderBlockCausal3D":
return DownEncoderBlockCausal3D(
num_layers=num_layers,
in_channels=in_channels,
out_channels=out_channels,
dropout=dropout,
add_downsample=add_downsample,
downsample_stride=downsample_stride,
resnet_eps=resnet_eps,
resnet_act_fn=resnet_act_fn,
resnet_groups=resnet_groups,
downsample_padding=downsample_padding,
resnet_time_scale_shift=resnet_time_scale_shift,
)
raise ValueError(f"{down_block_type} does not exist.")

def get_up_block3d(
up_block_type: str,
num_layers: int,
in_channels: int,
out_channels: int,
prev_output_channel: int,
temb_channels: int,
add_upsample: bool,
upsample_scale_factor: Tuple,
resnet_eps: float,
resnet_act_fn: str,
resolution_idx: Optional[int] = None,
transformer_layers_per_block: int = 1,
num_attention_heads: Optional[int] = None,
resnet_groups: Optional[int] = None,
cross_attention_dim: Optional[int] = None,
dual_cross_attention: bool = False,
use_linear_projection: bool = False,
only_cross_attention: bool = False,
upcast_attention: bool = False,
resnet_time_scale_shift: str = "default",
attention_type: str = "default",
resnet_skip_time_act: bool = False,
resnet_out_scale_factor: float = 1.0,
cross_attention_norm: Optional[str] = None,
attention_head_dim: Optional[int] = None,
upsample_type: Optional[str] = None,
dropout: float = 0.0,
) -> nn.Module:
# If attn head dim is not defined, we default it to the number of heads
if attention_head_dim is None:
logger_.warning(
f"It is recommended to provide attention_head_dim when calling get_up_block. Defaulting attention_head_dim to {num_attention_heads}."
)
attention_head_dim = num_attention_heads

up_block_type = up_block_type[7:] if up_block_type.startswith("UNetRes") else up_block_type
if up_block_type == "UpDecoderBlockCausal3D":
return UpDecoderBlockCausal3D(
num_layers=num_layers,
in_channels=in_channels,
out_channels=out_channels,
resolution_idx=resolution_idx,
dropout=dropout,
add_upsample=add_upsample,
upsample_scale_factor=upsample_scale_factor,
resnet_eps=resnet_eps,
resnet_act_fn=resnet_act_fn,
resnet_groups=resnet_groups,
resnet_time_scale_shift=resnet_time_scale_shift,
temb_channels=temb_channels,
)
raise ValueError(f"{up_block_type} does not exist.")

class UNetMidBlockCausal3D(nn.Module):
"""
中间模块，包含 (num_layers + 1) 个 ResnetBlockCausal3D 以及可选多头 Attention。
"""

def __init__(
self,
in_channels: int,
temb_channels: int,
dropout: float = 0.0,
num_layers: int = 1,
resnet_eps: float = 1e-6,
resnet_time_scale_shift: str = "default", # default, spatial
resnet_act_fn: str = "swish",
resnet_groups: int = 32,
attn_groups: int = None,
resnet_pre_norm: bool = True,
add_attention: bool = True,
attention_head_dim: int = 1,
output_scale_factor: float = 1.0,
):
super().__init__()
self.add_attention = add_attention
resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)

if attn_groups is None:
attn_groups = resnet_groups if resnet_time_scale_shift == "default" else None

# 第 0 个 Resnet
resnets = [
ResnetBlockCausal3D(
in_channels=in_channels,
out_channels=in_channels,
temb_channels=temb_channels,
eps=resnet_eps,
groups=resnet_groups,
dropout=dropout,
time_embedding_norm=resnet_time_scale_shift,
non_linearity=resnet_act_fn,
output_scale_factor=output_scale_factor,
pre_norm=resnet_pre_norm,
)
]
attentions = []

if attention_head_dim is None:
logger_.warning(
f"It is not recommended to pass attention_head_dim=None. Defaulting attention_head_dim to {in_channels}."
)
attention_head_dim = in_channels

# 后面 num_layers 次：Attention + Resnet
for _ in range(num_layers):
if self.add_attention:
attentions.append(
Attention(
in_channels,
heads=in_channels attention_head_dim,
dim_head=attention_head_dim,
rescale_output_factor=output_scale_factor,
eps=resnet_eps,
norm_num_groups=attn_groups,
spatial_norm_dim=temb_channels if resnet_time_scale_shift == "spatial" else None,
residual_connection=True,
bias=True,
upcast_softmax=True,
_from_deprecated_attn_block=True,
)
)
else:
attentions.append(None)

resnets.append(
ResnetBlockCausal3D(
in_channels=in_channels,
out_channels=in_channels,
temb_channels=temb_channels,
eps=resnet_eps,
groups=resnet_groups,
dropout=dropout,
time_embedding_norm=resnet_time_scale_shift,
non_linearity=resnet_act_fn,
output_scale_factor=output_scale_factor,
pre_norm=resnet_pre_norm,
)
)

self.attentions = nn.ModuleList(attentions)
self.resnets = nn.ModuleList(resnets)

# 总计 (num_layers + 1) 个 ResnetBlock
self.num_resblocks = num_layers + 1

# 用于存储每个resnet是否在前/后做 pooling/padding
self.resnet_pool_configs = [None] * self.num_resblocks
self.resnet_pad_configs = [None] * self.num_resblocks

def apply_t_ops_config_midblock(self, config: dict):
if not isinstance(config, dict):
return

epb = config.get("enable_t_pool_before_block", [])
epa = config.get("enable_t_pool_after_block", [])

if any(len(lst) != self.num_resblocks for lst in [epb, epa]):
raise ValueError(
f"[UNetMidBlockCausal3D] T-ops config mismatch: we have {self.num_resblocks} ResnetBlock(s), "
f"but got list lengths: {list(map(len, [epb, epa]))}"
)

pool_k = config.get("pool_t_kernel", 2)
pool_s = config.get("pool_t_stride", 2)


for i in range(self.num_resblocks):
self.resnet_pool_configs[i] = {
"enable_before": epb[i],
"enable_after": epa[i],
"kernel": pool_k,
"stride": pool_s
}

def forward(self, hidden_states: torch.FloatTensor, temb: torch.FloatTensor = None) -> torch.FloatTensor:
"""
先经过第 0 个 ResnetBlock（无 attention），然后再重复(Attention + ResnetBlock) num_layers次。
在每个 ResnetBlock 前/后，根据 self.resnet_pool_configs[i] / self.resnet_pad_configs[i] 决定是否做 replicate pad + avg_pool3d。
"""
for i in range(self.num_resblocks):
# 如果这是第 0 个 resnet，就没有 attention；否则 i>=1 时注意 handle attention
if i > 0:
# 先做 attention
attn = self.attentions[i - 1] # 第 (i-1) 个 attention
if attn is not None:
B, C, T, H, W = hidden_states.shape
hidden_states = rearrange(hidden_states, "b c f h w -> b (f h w) c")
attention_mask = prepare_causal_attention_mask(T, H*W, hidden_states.dtype, hidden_states.device, batch_size=B)
hidden_states = attn(hidden_states, temb=temb, attention_mask=attention_mask)
hidden_states = rearrange(hidden_states, "b (f h w) c -> b c f h w", f=T, h=H, w=W)

pool_conf = self.resnet_pool_configs[i] or {}
if pool_conf.get("enable_before", False):
k, s = pool_conf["kernel"], pool_conf["stride"]
hidden_states = F.pad(hidden_states, pad=(0,0,0,0,k-1,0), mode='replicate')
hidden_states = F.avg_pool3d(hidden_states, kernel_size=(k,1,1), stride=(s,1,1))
# 再做 ResnetBlock
hidden_states = self.resnets[i](hidden_states, temb=temb)

# 是否在 ResnetBlock 之后做 pool/pad
if pool_conf.get("enable_after", False):
k, s = pool_conf["kernel"], pool_conf["stride"]
hidden_states = F.pad(hidden_states, pad=(0,0,0,0,k-1,0), mode='replicate')
hidden_states = F.avg_pool3d(hidden_states, kernel_size=(k,1,1), stride=(s,1,1))

return hidden_states

class DownEncoderBlockCausal3D(nn.Module):
def __init__(
self,
in_channels: int,
out_channels: int,
dropout: float = 0.0,
num_layers: int = 1,
resnet_eps: float = 1e-6,
resnet_time_scale_shift: str = "default",
resnet_act_fn: str = "swish",
resnet_groups: int = 32,
resnet_pre_norm: bool = True,
output_scale_factor: float = 1.0,
add_downsample: bool = True,
downsample_stride: int = 2,
downsample_padding: int = 1,
):
super().__init__()
resnets = []

for i in range(num_layers):
in_channels = in_channels if i == 0 else out_channels
resnets.append(
ResnetBlockCausal3D(
in_channels=in_channels,
out_channels=out_channels,
temb_channels=None,
eps=resnet_eps,
groups=resnet_groups,
dropout=dropout,
time_embedding_norm=resnet_time_scale_shift,
non_linearity=resnet_act_fn,
output_scale_factor=output_scale_factor,
pre_norm=resnet_pre_norm,
)
)

self.resnets = nn.ModuleList(resnets)

if add_downsample:
self.downsamplers = nn.ModuleList(
[
DownsampleCausal3D(
out_channels,
use_conv=True,
out_channels=out_channels,
padding=downsample_padding,
name="op",
stride=downsample_stride,
)
]
)
else:
self.downsamplers = None
self.resnet_pool_configs = [None] * num_layers

def apply_t_ops_config(self, block_config: dict):
if "downsample_stride" in block_config:
ds_stride = tuple(block_config["downsample_stride"]) # e.g. [1, 2, 2] -> (1, 2, 2)
if self.downsamplers is not None:
for ds in self.downsamplers:
ds.conv.conv.stride = ds_stride
_logger.info(f"Down Conv Stride updated to {ds_stride}")

num_resnet = len(self.resnets)
epb = block_config.get("enable_t_pool_before_block", [])
epa = block_config.get("enable_t_pool_after_block", [])
if any(len(x) != num_resnet for x in [epb, epa]):
raise ValueError(
f"[DownEncoderBlockCausal3D] config mismatch: expecting {num_resnet} bools in each list."
)

pool_k = block_config.get("pool_t_kernel", 2)
pool_s = block_config.get("pool_t_stride", 2)

for i in range(num_resnet):
pool_conf = {
"enable_before": epb[i],
"enable_after": epa[i],
"kernel": pool_k,
"stride": pool_s
}
self.resnet_pool_configs[i] = pool_conf

def forward(self, hidden_states: torch.FloatTensor, scale: float = 1.0, index: int = None) -> torch.FloatTensor:
for i, resnet in enumerate(self.resnets):
pool_conf = self.resnet_pool_configs[i] or {}
if pool_conf.get("enable_before", False):
k, s = pool_conf["kernel"], pool_conf["stride"]
_logger.info(f"Down Pooling before ResnetBlock: kernel={k}, stride={s}, hidden_states.shape={hidden_states.shape}, layer={i}, index={index}")
padding = (k-1, 0) # 仅在时间维度前向填充 k-1 个像素
hidden_states = F.pad(hidden_states, pad=(0, 0, 0, 0, padding[0], padding[1]), mode='replicate')
hidden_states = F.avg_pool3d(hidden_states, kernel_size=(k, 1, 1), stride=(s, 1, 1))

# 2) ResnetBlock
hidden_states = resnet(hidden_states, temb=None, scale=scale)

# 3) pool/pad AFTER
if pool_conf.get("enable_after", False):
k, s = pool_conf["kernel"], pool_conf["stride"]
padding = (k-1, 0) # 仅在时间维度前向填充 k-1 个像素
hidden_states = F.pad(hidden_states, pad=(0, 0, 0, 0, padding[0], padding[1]), mode='replicate')
hidden_states = F.avg_pool3d(hidden_states, kernel_size=(k, 1, 1), stride=(s, 1, 1))
_logger.info(f"Down Pooling after ResnetBlock: kernel={k}, stride={s}, hidden_states.shape={hidden_states.shape}, layer={i}, index={index}")

# 下采样
if self.downsamplers is not None:
for downsampler in self.downsamplers:
hidden_states = downsampler(hidden_states, scale)

return hidden_states

class UpDecoderBlockCausal3D(nn.Module):
"""
上采样解码块, 包含 num_layers 个 ResnetBlockCausal3D, 以及可选 UpsampleCausal3D。
支持在每个 resnet 前/后插入 T 维度 pool/pad。
"""
def __init__(
self,
in_channels: int,
out_channels: int,
resolution_idx: Optional[int] = None,
dropout: float = 0.0,
num_layers: int = 1,
resnet_eps: float = 1e-6,
resnet_time_scale_shift: str = "default", # default, spatial
resnet_act_fn: str = "swish",
resnet_groups: int = 32,
resnet_pre_norm: bool = True,
output_scale_factor: float = 1.0,
add_upsample: bool = True,
upsample_scale_factor=(2, 2, 2),
temb_channels: Optional[int] = None,
):
super().__init__()
resnets = []
for i in range(num_layers):
block_in_ch = in_channels if i == 0 else out_channels
block = ResnetBlockCausal3D(
in_channels=block_in_ch,
out_channels=out_channels,
temb_channels=temb_channels,
eps=resnet_eps,
groups=resnet_groups,
dropout=dropout,
time_embedding_norm=resnet_time_scale_shift,
non_linearity=resnet_act_fn,
output_scale_factor=output_scale_factor,
pre_norm=resnet_pre_norm,
)
resnets.append(block)
self.resnets = nn.ModuleList(resnets)

if add_upsample:
self.upsamplers = nn.ModuleList([
UpsampleCausal3D(
out_channels,
use_conv=True,
out_channels=out_channels,
upsample_factor=upsample_scale_factor,
)
])
else:
self.upsamplers = None

self.resolution_idx = resolution_idx

# 为每个 ResnetBlock 存储 pool/pad 配置
self.resnet_interp_configs = [None] * num_layers
self.resolution_idx = resolution_idx
self.num_layers = num_layers


def apply_t_ops_config(self, block_config: dict):
num_resnet = len(self.resnets)
eib = block_config.get("enable_t_interp_before_block", [])
eia = block_config.get("enable_t_interp_after_block", [])
if any(len(x) != num_resnet for x in [eib, eia]):
raise ValueError(
f"[UpDecoderBlockCausal3D] config mismatch: expecting {num_resnet} bools in each list."
)

interp_scale = block_config.get("interp_t_scale_factor", 2)
interp_mode = block_config.get("interp_mode", "nearest")
for i in range(num_resnet):
interp_conf = {
"enable_before": eib[i],
"enable_after": eia[i],
"scale_factor": interp_scale,
"mode": interp_mode
}
self.resnet_interp_configs[i] = interp_conf

def forward(
self, hidden_states: torch.FloatTensor, temb: Optional[torch.FloatTensor] = None, scale: float = 1.0
) -> torch.FloatTensor:
"""
在每个 ResnetBlock 前/后，看情况做:
- avg_pool3d (pool)
- replicate pad (pad)
- F.interpolate (插值)
然后再做 block 本身。
"""
for i, resnet in enumerate(self.resnets):
interp_conf = self.resnet_interp_configs[i] or {}

# 1) 先看看有没有 "插值前"：
if interp_conf.get("enable_before", False):
sc = interp_conf["scale_factor"]
mode = interp_conf["mode"]
# 只对 time 维度插值 (dim=2)
# nearest插值非平滑, 但支持反向传播
if hidden_states.shape[2] > 0:
hidden_states = F.interpolate(
hidden_states,
scale_factor=(sc, 1, 1), # 仅沿时间维度放大 sc 倍
mode=mode
)
_logger.info(f"Up Interp before ResnetBlock: scale_factor={sc,1,1},hidden_states.shape={hidden_states.shape}, layer={i}")

hidden_states = resnet(hidden_states, temb=temb, scale=scale)

if interp_conf.get("enable_after", False):
sc = interp_conf["scale_factor"]
mode = interp_conf["mode"]
if hidden_states.shape[2] > 0:
hidden_states = F.interpolate(
hidden_states,
scale_factor=(sc, 1, 1),
mode=mode
)
_logger.info(f"Up Interp before ResnetBlock: scale_factor={sc,1,1},hidden_states.shape={hidden_states.shape}, layer={i}")
# upsample (原逻辑)
if self.upsamplers is not None:
for upsampler in self.upsamplers:
hidden_states = upsampler(hidden_states)

return hidden_states
vae.py内容是:from dataclasses import dataclass
from typing import Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
from loguru import logger
from diffusers.utils import BaseOutput, is_torch_version
from diffusers.utils.torch_utils import randn_tensor
from diffusers.models.attention_processor import SpatialNorm
from .unet_causal_3d_blocks import (
CausalConv3d,
UNetMidBlockCausal3D,
get_down_block3d,
get_up_block3d,
)


@dataclass
class DecoderOutput(BaseOutput):
r"""
Output of decoding method.

Args:
sample (torch.FloatTensor of shape (batch_size, num_channels, height, width)):
The decoded output sample from the last layer of the model.
"""

sample: torch.FloatTensor


class EncoderCausal3D(nn.Module):
r"""
The EncoderCausal3D layer of a variational autoencoder that encodes its input into a latent representation.
"""

def __init__(
self,
in_channels: int = 3,
out_channels: int = 3,
down_block_types: Tuple[str, ...] = ("DownEncoderBlockCausal3D",),
block_out_channels: Tuple[int, ...] = (64,),
layers_per_block: int = 2,
norm_num_groups: int = 32,
act_fn: str = "silu",
double_z: bool = True,
mid_block_add_attention=True,
time_compression_ratio: int = 4,
spatial_compression_ratio: int = 8,
):
super().__init__()
self.layers_per_block = layers_per_block

self.conv_in = CausalConv3d(in_channels, block_out_channels[0], kernel_size=3, stride=1)
self.mid_block = None
self.down_blocks = nn.ModuleList([])

# down
output_channel = block_out_channels[0]
for i, down_block_type in enumerate(down_block_types):
input_channel = output_channel#每个块的输入通道是上一个块的输出通道。
output_channel = block_out_channels[i]
is_final_block = i == len(block_out_channels) - 1
#下采样的层数
num_spatial_downsample_layers = int(np.log2(spatial_compression_ratio))
num_time_downsample_layers = int(np.log2(time_compression_ratio))

if time_compression_ratio == 4:
#空间下采样层层有
add_spatial_downsample = bool(i < num_spatial_downsample_layers)
#时间下采样通常安排在编码器的后几层
add_time_downsample = bool(
i >= (len(block_out_channels) - 1 - num_time_downsample_layers)
and not is_final_block
)
else:
raise ValueError(f"Unsupported time_compression_ratio: {time_compression_ratio}.")

downsample_stride_HW = (2, 2) if add_spatial_downsample else (1, 1)
downsample_stride_T = (2,) if add_time_downsample else (1,)
downsample_stride = tuple(downsample_stride_T + downsample_stride_HW)
down_block = get_down_block3d(
down_block_type,
num_layers=self.layers_per_block,
in_channels=input_channel,
out_channels=output_channel,
add_downsample=bool(add_spatial_downsample or add_time_downsample),#最后进行下空间下采样,以及空间下采样后的时间下采样,通过传入stride的tuple进行控制
downsample_stride=downsample_stride,
resnet_eps=1e-6,
downsample_padding=0,#没用上这个参数
resnet_act_fn=act_fn,
resnet_groups=norm_num_groups,
attention_head_dim=output_channel,
temb_channels=None,
)
self.down_blocks.append(down_block)

# mid
self.mid_block = UNetMidBlockCausal3D(
in_channels=block_out_channels[-1],
resnet_eps=1e-6,
resnet_act_fn=act_fn,
output_scale_factor=1,
resnet_time_scale_shift="default",
attention_head_dim=block_out_channels[-1],
resnet_groups=norm_num_groups,
temb_channels=None,
add_attention=mid_block_add_attention,
)

# out
self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-6)
self.conv_act = nn.SiLU()

conv_out_channels = 2 * out_channels if double_z else out_channels
self.conv_out = CausalConv3d(block_out_channels[-1], conv_out_channels, kernel_size=3) #stride还是1

def forward(self, sample: torch.FloatTensor) -> torch.FloatTensor:
r"""The forward method of the EncoderCausal3D class."""
assert len(sample.shape) == 5, "The input tensor should have 5 dimensions"

sample = self.conv_in(sample) #仅channel纬度发生变化,in_channels -> block_out_channels[0]

# down
for i, down_block in enumerate(self.down_blocks): #改成
sample = down_block(sample, index=i)

# middle
sample = self.mid_block(sample)

# post-process
sample = self.conv_norm_out(sample) #channel维度GroupNorm
sample = self.conv_act(sample)
sample = self.conv_out(sample) #仅channel纬度发生变化,block_out_channels[-1] -> conv_out_channels

return sample


class DecoderCausal3D(nn.Module):
r"""
The DecoderCausal3D layer of a variational autoencoder that decodes its latent representation into an output sample.
"""

def __init__(
self,
in_channels: int = 3,
out_channels: int = 3,
up_block_types: Tuple[str, ...] = ("UpDecoderBlockCausal3D",),
block_out_channels: Tuple[int, ...] = (64,),
layers_per_block: int = 2,
norm_num_groups: int = 32,
act_fn: str = "silu",
norm_type: str = "group", # group, spatial
mid_block_add_attention=True,
time_compression_ratio: int = 4,
spatial_compression_ratio: int = 8,
):
super().__init__()
self.layers_per_block = layers_per_block

self.conv_in = CausalConv3d(in_channels, block_out_channels[-1], kernel_size=3, stride=1)
self.mid_block = None
self.up_blocks = nn.ModuleList([])

temb_channels = in_channels if norm_type == "spatial" else None

# mid
self.mid_block = UNetMidBlockCausal3D(
in_channels=block_out_channels[-1],
resnet_eps=1e-6,
resnet_act_fn=act_fn,
output_scale_factor=1,
resnet_time_scale_shift="default" if norm_type == "group" else norm_type,
attention_head_dim=block_out_channels[-1],
resnet_groups=norm_num_groups,
temb_channels=temb_channels, #与encoder的区别 也是额外传入了temb_channels 而像素的channel一样 ,主要是应用于ResnetBlockCausal3D中,卷积的之前与之后
add_attention=mid_block_add_attention,
)

# up
reversed_block_out_channels = list(reversed(block_out_channels))
output_channel = reversed_block_out_channels[0]
for i, up_block_type in enumerate(up_block_types):
prev_output_channel = output_channel
output_channel = reversed_block_out_channels[i]
is_final_block = i == len(block_out_channels) - 1
num_spatial_upsample_layers = int(np.log2(spatial_compression_ratio))
num_time_upsample_layers = int(np.log2(time_compression_ratio))

if time_compression_ratio == 4:
add_spatial_upsample = bool(i < num_spatial_upsample_layers)
add_time_upsample = bool(
i >= len(block_out_channels) - 1 - num_time_upsample_layers
and not is_final_block
)
else:
raise ValueError(f"Unsupported time_compression_ratio: {time_compression_ratio}.")

upsample_scale_factor_HW = (2, 2) if add_spatial_upsample else (1, 1)
upsample_scale_factor_T = (2,) if add_time_upsample else (1,)
upsample_scale_factor = tuple(upsample_scale_factor_T + upsample_scale_factor_HW)
up_block = get_up_block3d(
up_block_type,
num_layers=self.layers_per_block + 1, #额外多了一层 encoder中的layer是指ResnetBlockCausal3D个数 这里同样 但是多了一层
in_channels=prev_output_channel,
out_channels=output_channel,
prev_output_channel=None,
add_upsample=bool(add_spatial_upsample or add_time_upsample),
upsample_scale_factor=upsample_scale_factor, #跟down的stride是一个参数
resnet_eps=1e-6,
resnet_act_fn=act_fn,
resnet_groups=norm_num_groups,
attention_head_dim=output_channel,
temb_channels=temb_channels, #传入了实际的数值 temb 用于对激活值进行norm 又多种norm方法 1.scale,shift ada 2. 直接相加 default 等等 这个比较多 主要目的就是norm
resnet_time_scale_shift=norm_type,
)
self.up_blocks.append(up_block)
prev_output_channel = output_channel

# out
if norm_type == "spatial":
self.conv_norm_out = SpatialNorm(block_out_channels[0], temb_channels)
else:
self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=norm_num_groups, eps=1e-6)
self.conv_act = nn.SiLU()
self.conv_out = CausalConv3d(block_out_channels[0], out_channels, kernel_size=3)

self.gradient_checkpointing = False

def forward(
self,
sample: torch.FloatTensor,
latent_embeds: Optional[torch.FloatTensor] = None,
) -> torch.FloatTensor:
r"""The forward method of the DecoderCausal3D class."""
assert len(sample.shape) == 5, "The input tensor should have 5 dimensions."

sample = self.conv_in(sample)

upscale_dtype = next(iter(self.up_blocks.parameters())).dtype
if self.training and self.gradient_checkpointing:

def create_custom_forward(module):
def custom_forward(*inputs):
return module(*inputs)

return custom_forward

if is_torch_version(">=", "1.11.0"):
# middle
sample = torch.utils.checkpoint.checkpoint(
create_custom_forward(self.mid_block),
sample,
latent_embeds,
use_reentrant=False,
)
sample = sample.to(upscale_dtype)

# up
for up_block in self.up_blocks:
sample = torch.utils.checkpoint.checkpoint(
create_custom_forward(up_block),
sample,
latent_embeds,
use_reentrant=False,
)
else:
# middle
sample = torch.utils.checkpoint.checkpoint(
create_custom_forward(self.mid_block), sample, latent_embeds
)
sample = sample.to(upscale_dtype)

# up
for up_block in self.up_blocks:
sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds)
else:
# middle
sample = self.mid_block(sample, latent_embeds)
sample = sample.to(upscale_dtype)

# up
for up_block in self.up_blocks:
sample = up_block(sample, latent_embeds)

# post-process
if latent_embeds is None:
sample = self.conv_norm_out(sample)
else:
sample = self.conv_norm_out(sample, latent_embeds)
sample = self.conv_act(sample)
sample = self.conv_out(sample)

return sample


class DiagonalGaussianDistribution(object):
def __init__(self, parameters: torch.Tensor, deterministic: bool = False):
if parameters.ndim == 3:
dim = 2 # (B, L, C)
elif parameters.ndim == 5 or parameters.ndim == 4:
dim = 1 # (B, C, T, H ,W) / (B, C, H, W)
else:
raise NotImplementedError
self.parameters = parameters
self.mean, self.logvar = torch.chunk(parameters, 2, dim=dim)
self.logvar = torch.clamp(self.logvar, -30.0, 20.0) #避免对数方差的值过小或过大导致数值不稳定
self.deterministic = deterministic
self.std = torch.exp(0.5 * self.logvar)
self.var = torch.exp(self.logvar)
if self.deterministic:
self.var = self.std = torch.zeros_like(
self.mean, device=self.parameters.device, dtype=self.parameters.dtype
)
#从指定分布上进行采样
def sample(self, generator: Optional[torch.Generator] = None) -> torch.FloatTensor:
# make sure sample is on the same device as the parameters and has same dtype
sample = randn_tensor(
self.mean.shape,
generator=generator,
device=self.parameters.device,
dtype=self.parameters.dtype,
)
x = self.mean + self.std * sample
return x

def kl(self, other: "DiagonalGaussianDistribution" = None) -> torch.Tensor:
if self.deterministic:
return torch.Tensor([0.0])
else:
reduce_dim = list(range(1, self.mean.ndim))
if other is None: #跟标准正态分布进行比较
return 0.5 * torch.sum(
torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar,
dim=reduce_dim,
)
else:#跟已知分布进行比较 多维高斯 mean和std都是多维度 要求除了batch之外,t,h,w,c求和
return 0.5 * torch.sum(
torch.pow(self.mean - other.mean, 2) / other.var
+ self.var / other.var
- 1.0
- self.logvar
+ other.logvar,
dim=reduce_dim,
)

def nll(self, sample: torch.Tensor, dims: Tuple[int, ...] = [1, 2, 3]) -> torch.Tensor:
if self.deterministic:
return torch.Tensor([0.0])
logtwopi = np.log(2.0 * np.pi)
return 0.5 * torch.sum(
logtwopi + self.logvar +
torch.pow(sample - self.mean, 2) / self.var,
dim=dims,
)

def mode(self) -> torch.Tensor:
return self.mean
读取的json文件是:t_ops_config.json:{
"encoder": {
"down_blocks": [
{
"block_type": "DownEncoderBlockCausal3D",
"block_index": 0,
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false],
"downsample_stride": [1, 2, 2]
},
{
"block_type": "DownEncoderBlockCausal3D",
"block_index": 1,
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false],
"downsample_stride": [2, 2, 2]
},
{
"block_type": "DownEncoderBlockCausal3D",
"block_index": 2,
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false],
"downsample_stride": [2, 2, 2]
},
{
"block_type": "DownEncoderBlockCausal3D",
"block_index": 3,
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false],
"downsample_stride": [1, 1, 1]
}
],
"mid_block": {
"mid_block_type": "UNetMidBlockCausal3D",
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false]
}
},

"decoder": {
"up_blocks": [
{
"block_type": "UpDecoderBlockCausal3D",
"block_index": 0,
"enable_t_interp_before_block": [false, false, false],
"enable_t_interp_after_block": [false, false, false],
"interp_t_scale_factor": 2,
"interp_mode": "nearest"
},
{
"block_type": "UpDecoderBlockCausal3D",
"block_index": 1,
"enable_t_interp_before_block": [false, false, false],
"enable_t_interp_after_block": [false, false, false],
"interp_t_scale_factor": 2,
"interp_mode": "nearest"
},
{
"block_type": "UpDecoderBlockCausal3D",
"block_index": 2,
"enable_t_interp_before_block": [false, false, false],
"enable_t_interp_after_block": [false, false, false],
"interp_t_scale_factor": 2,
"interp_mode": "nearest"
},
{
"block_type": "UpDecoderBlockCausal3D",
"block_index": 3,
"enable_t_interp_before_block": [false, false, false],
"enable_t_interp_after_block": [false, false, false],
"interp_t_scale_factor": 2,
"interp_mode": "nearest"
}

],
"mid_block": {
"mid_block_type": "UNetMidBlockCausal3D",
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false]
}
}
}
我想再次基础上修改代码,在autoencoder_kl_causal_3d.py中构建一个类,这类里面有多种方式可以计算temporal_tiled_encode函数中每个在frame纬度切分后的tile的信息量,比如我可以用ffprobe计算每个tile的平均码率(你先实现这个方案,除此之外  你可以再实现两种方案,给我留个def pass这种函数 方便我后续自动以计算信息量的方法,这个函数要求读取tile输出属于哪个档),然后根据码率是属于1000以内,还是1000-2000,2000以上分为三档,来决定当前tile的压缩率,0~1000压缩率是4倍,1000~2000是2倍,2000以上不压缩.对应的你就可以load不同的json文件,4倍的json文件路是/home/hanling/HunyuanVideo_efficiency/analysis/config_stride2_json/exp_262.json,2倍的json文件路径是:/home/hanling/HunyuanVideo_efficiency/analysis/config_stride_json/exp_20.json,你考考虑下不同压缩是不是需要修改tiled_encode,decode,blend等函数 以求拼接上不要出什么问题