ç¨‹åºå…¥å£æ˜¯:infer.py:#!/usr/bin/env python3
# infer.py

import argparse
import os
import torch
from torch.utils.data import DataLoader
from loguru import logger

from dataset_processor.dataset_loader import VideoTensorDataset
from hyvideo.vae.autoencoder_kl_causal_3d import AutoencoderKLCausal3D
from hyvideo.vae import load_vae
from hyvideo.utils.file_utils import save_videos_grid


def save_model_architecture_to_file(module, file_path: str):
"""å°†æ¨¡å‹æ¶æ„ä¿å­˜åˆ°æ–‡ä»¶ï¼Œç¤ºä¾‹åŠŸèƒ½ä¸å˜ï¼Œå¯é€‰ã€‚"""
with open(file_path, "w") as f:
def write_full_model(module, indent: int = 0):
prefix = " " * indent
f.write(f"{prefix}{module.__class__.__name__}:\n")
for name, sub_module in module.named_children():
f.write(f"{prefix} ({name}): {sub_module}\n")
write_full_model(sub_module, indent + 4)
write_full_model(module)


def infer_vae(model: AutoencoderKLCausal3D,
dataloader: DataLoader,
device: str,
output_dir: str,
max_files: int = None,
mp4: bool = False):
"""
Perform inference using the VAE model on video tensors.
"""
model.to(device)
model.eval()

os.makedirs(output_dir, exist_ok=True)

for batch_idx, (video_tensor, file_name) in enumerate(dataloader):
if max_files is not None and batch_idx >= max_files:
break # Stop processing after reaching the max number of files

# å»æ‰ .pt åç¼€
file_name = file_name[0].replace(".pt", "")

# Move to device
video_tensor = video_tensor.to(device, dtype=torch.float16)
logger.info(f"Processing {file_name}, video shape: {video_tensor.shape}")

with torch.no_grad():
# Encode and decode video
reconstructed_video = model(
video_tensor,
return_dict=False,
return_posterior=True,
sample_posterior=False
)[0]

# Save the reconstructed video in .pt format
reconstructed_video = reconstructed_video.cpu().float()
output_path = os.path.join(output_dir, f"{file_name}.pt")
torch.save(reconstructed_video, output_path)
logger.info(f"Saved reconstructed video to {output_path}, shape: {reconstructed_video.shape}")

# Optionally save mp4
if mp4:
save_path = os.path.join(output_dir, f"{file_name}.mp4")
save_videos_grid(reconstructed_video, save_path, fps=24, rescale=True)
logger.info(f'Sample saved to: {save_path}')


def parse_args():
parser = argparse.ArgumentParser(description="VAE Inference script for video tensors.")
parser.add_argument("--tensor-dir", type=str, required=True,
help="Directory containing input .pt video tensors.")
parser.add_argument("--output-dir", type=str, required=True,
help="Directory to save the reconstructed videos.")
parser.add_argument("--vae-path", type=str, default="ckpts/hunyuan-video-t2v-720p/vae",
help="Path to VAE checkpoint directory (contains pytorch_model.pt).")
parser.add_argument("--config-json", type=str, default="t_ops_config.json",
help="Path to the T-ops config JSON file.")
parser.add_argument("--max-files", type=int, default=None,
help="Max number of input files to process (for quick testing).")
parser.add_argument("--mp4", action="store_true",
help="If set, also save outputs as .mp4 videos.")
parser.add_argument("--batch-size", type=int, default=1,
help="Batch size for data loader.")
parser.add_argument("--num-workers", type=int, default=4,
help="Number of workers for data loader.")
return parser.parse_args()


def main():
args = parse_args()
logger.info(f"Running inference with args: {args}")

device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½ VAE æ¨¡å‹
logger.info("Loading VAE...")
vae, _, s_ratio, t_ratio = load_vae(
vae_type="884-16c-hy", # è¿™é‡Œå’Œä½ ä»¬é¡¹ç›®é‡Œä¿æŒä¸€è‡´
vae_precision="fp16",
logger=logger,
vae_path=args.vae_path,
device=device,
t_ops_config_path=args.config_json,
test=True
)
logger.info("VAE loaded.")

# å¦‚æœä½ æƒ³å¯ç”¨ tilingï¼Œå¯åœ¨æ­¤å¤„å¯ç”¨
vae.enable_tiling()

# åŠ è½½æ•°æ®é›†
dataset = VideoTensorDataset(args.tensor_dir)
dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)

# è¿è¡Œæ¨ç†
infer_vae(vae, dataloader, device, args.output_dir, max_files=args.max_files, mp4=args.mp4)


if __name__ == "__main__":
main()
__init__.pyå†…å®¹æ˜¯:from pathlib import Path

import torch

from .autoencoder_kl_causal_3d import AutoencoderKLCausal3D
from ..constants import VAE_PATH, PRECISION_TO_TYPE

from pathlib import Path
from loguru import logger
import torch
import json
from .autoencoder_kl_causal_3d import AutoencoderKLCausal3D
from ..constants import VAE_PATH, PRECISION_TO_TYPE

def _apply_t_ops_config_to_vae(vae: AutoencoderKLCausal3D, t_ops_config: dict):
"""
å°† t_ops_config æ³¨å…¥åˆ° vae.encoder.down_blocks / vae.encoder.mid_block / vae.decoder.up_blocks / vae.decoder.mid_block.
"""
# -------------- 1) encoder --------------
enc_cfg = t_ops_config.get("encoder", {})

# (1a) éå† down_blocks
down_blocks_cfg = enc_cfg.get("down_blocks", [])
for block_cfg in down_blocks_cfg:
idx = block_cfg["block_index"]
if 0 <= idx < len(vae.encoder.down_blocks):
down_block = vae.encoder.down_blocks[idx]
if hasattr(down_block, "apply_t_ops_config"):
down_block.apply_t_ops_config(block_cfg)
else:
print(f"[Warning] down_block at index {idx} lacks apply_t_ops_config().")
else:
print(f"[Warning] down_block index {idx} out of range of encoder.down_blocks.")

# (1b) encoder.mid_block
enc_mid_cfg = enc_cfg.get("mid_block", {}) # <=== æ–°å¢
if hasattr(vae.encoder, "mid_block") and hasattr(vae.encoder.mid_block, "apply_t_ops_config_midblock"):
vae.encoder.mid_block.apply_t_ops_config_midblock(enc_mid_cfg)
else:
print("[Warning] encoder.mid_block not found or has no apply_t_ops_config_midblock method.")

# -------------- 2) decoder --------------
dec_cfg = t_ops_config.get("decoder", {})

# (2a) éå† up_blocks
up_blocks_cfg = dec_cfg.get("up_blocks", [])
for block_cfg in up_blocks_cfg:
idx = block_cfg["block_index"]
if 0 <= idx < len(vae.decoder.up_blocks):
up_block = vae.decoder.up_blocks[idx]
if hasattr(up_block, "apply_t_ops_config"):
up_block.apply_t_ops_config(block_cfg)
else:
print(f"[Warning] up_block at index {idx} lacks apply_t_ops_config().")
else:
print(f"[Warning] up_block index {idx} out of range of decoder.up_blocks.")

# (2b) decoder.mid_block
dec_mid_cfg = dec_cfg.get("mid_block", {}) # <=== æ–°å¢
if hasattr(vae.decoder, "mid_block") and hasattr(vae.decoder.mid_block, "apply_t_ops_config_midblock"):
vae.decoder.mid_block.apply_t_ops_config_midblock(dec_mid_cfg)
else:
print("[Warning] decoder.mid_block not found or has no apply_t_ops_config_midblock method.")


def load_t_ops_config(json_path: str) -> dict:
with open(json_path, "r") as f:
return json.load(f)

def load_vae(
vae_type: str="884-16c-hy",
vae_precision: str=None,
sample_size: tuple=None,
vae_path: str=None,
logger=None,
device=None,
t_ops_config_path: str = None,
test: bool = False,
):
"""
Load the 3D VAE model.
"""
if vae_path is None:
vae_path = VAE_PATH[vae_type]

if logger is not None:
logger.info(f"Loading 3D VAE model ({vae_type}) from: {vae_path}")
config = AutoencoderKLCausal3D.load_config(vae_path)
if sample_size:
vae = AutoencoderKLCausal3D.from_config(config, sample_size=sample_size)
else:
vae = AutoencoderKLCausal3D.from_config(config)

vae_ckpt = Path(vae_path) / "pytorch_model.pt"
assert vae_ckpt.exists(), f"VAE checkpoint not found: {vae_ckpt}"

ckpt = torch.load(vae_ckpt, map_location=vae.device, weights_only=False)
if "state_dict" in ckpt:
ckpt = ckpt["state_dict"]
if any(k.startswith("vae.") for k in ckpt.keys()):
ckpt = {k.replace("vae.", ""): v for k, v in ckpt.items() if k.startswith("vae.")}
vae.load_state_dict(ckpt)

spatial_compression_ratio = vae.config.spatial_compression_ratio
time_compression_ratio = vae.config.time_compression_ratio

if vae_precision is not None:
vae = vae.to(dtype=PRECISION_TO_TYPE[vae_precision])

vae.requires_grad_(False)

if logger is not None:
logger.info(f"VAE to dtype: {vae.dtype}")

if device is not None:
vae = vae.to(device)

vae.eval()

# ============ å¦‚æœä¼ å…¥äº† t_ops_config_pathï¼Œå¹¶ä¸” test=Trueï¼Œå°±è°ƒç”¨ _apply_t_ops_config_to_vae ============
if t_ops_config_path is not None and test:
my_t_ops_config = load_t_ops_config(t_ops_config_path)
if logger is not None:
logger.info("Applying T-pool/pad configs to the loaded VAE.")
_apply_t_ops_config_to_vae(vae, my_t_ops_config)

return vae, vae_path, spatial_compression_ratio, time_compression_ratio
autoencoder_ki_causal_3d.pyå†…å®¹æ˜¯:# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
#
# Modified from diffusers==0.29.2
#
# ==============================================================================
from typing import Dict, Optional, Tuple, Union
from dataclasses import dataclass
from loguru import logger
import torch
import torch.nn as nn

from diffusers.configuration_utils import ConfigMixin, register_to_config

try:
# This diffusers is modified and packed in the mirror.
from diffusers.loaders import FromOriginalVAEMixin
except ImportError:
# Use this to be compatible with the original diffusers.
from diffusers.loaders.single_file_model import FromOriginalModelMixin as FromOriginalVAEMixin
from diffusers.utils.accelerate_utils import apply_forward_hook
from diffusers.models.attention_processor import (
ADDED_KV_ATTENTION_PROCESSORS,
CROSS_ATTENTION_PROCESSORS,
Attention,
AttentionProcessor,
AttnAddedKVProcessor,
AttnProcessor,
)
from diffusers.models.modeling_outputs import AutoencoderKLOutput
from diffusers.models.modeling_utils import ModelMixin
from .vae import DecoderCausal3D, BaseOutput, DecoderOutput, DiagonalGaussianDistribution, EncoderCausal3D


@dataclass
class DecoderOutput2(BaseOutput):
sample: torch.FloatTensor
posterior: Optional[DiagonalGaussianDistribution] = None


class AutoencoderKLCausal3D(ModelMixin, ConfigMixin, FromOriginalVAEMixin):
r"""
A VAE model with KL loss for encoding images/videos into latents and decoding latent representations into images/videos.

This model inherits from [ModelMixin]. Check the superclass documentation for it's generic methods implemented
for all models (such as downloading or saving).
"""

_supports_gradient_checkpointing = True

@register_to_config
def __init__(
self,
in_channels: int = 3,
out_channels: int = 3,
down_block_types: Tuple[str] = ("DownEncoderBlockCausal3D",),
up_block_types: Tuple[str] = ("UpDecoderBlockCausal3D",),
block_out_channels: Tuple[int] = (64,),
layers_per_block: int = 1,
act_fn: str = "silu",
latent_channels: int = 4,
norm_num_groups: int = 32,
sample_size: int = 32,
sample_tsize: int = 64,
scaling_factor: float = 0.18215,
force_upcast: float = True,
spatial_compression_ratio: int = 8,
time_compression_ratio: int = 4,
mid_block_add_attention: bool = True,
):
super().__init__()

self.time_compression_ratio = time_compression_ratio

self.encoder = EncoderCausal3D(
in_channels=in_channels,
out_channels=latent_channels,
down_block_types=down_block_types,
block_out_channels=block_out_channels,
layers_per_block=layers_per_block,
act_fn=act_fn,
norm_num_groups=norm_num_groups,
double_z=True, #å¦‚æœä¸ºTrueï¼Œå°†zçš„ç»´åº¦æ‰©å¤§ä¸€å€,2*latent_channels
time_compression_ratio=time_compression_ratio,
spatial_compression_ratio=spatial_compression_ratio,
mid_block_add_attention=mid_block_add_attention,
)

self.decoder = DecoderCausal3D(
in_channels=latent_channels,
out_channels=out_channels,
up_block_types=up_block_types,
block_out_channels=block_out_channels,
layers_per_block=layers_per_block,
norm_num_groups=norm_num_groups,
act_fn=act_fn,
time_compression_ratio=time_compression_ratio,
spatial_compression_ratio=spatial_compression_ratio,
mid_block_add_attention=mid_block_add_attention,
)
#åªæ˜¯ç®€å•çš„çº¿æ€§å˜åŒ–
self.quant_conv = nn.Conv3d(2 * latent_channels, 2 * latent_channels, kernel_size=1)
self.post_quant_conv = nn.Conv3d(latent_channels, latent_channels, kernel_size=1)

self.use_slicing = False
self.use_spatial_tiling = False
self.use_temporal_tiling = False

# only relevant if vae tiling is enabled
self.tile_sample_min_tsize = sample_tsize
self.tile_latent_min_tsize = sample_tsize // time_compression_ratio

self.tile_sample_min_size = self.config.sample_size
sample_size = (
self.config.sample_size[0]
if isinstance(self.config.sample_size, (list, tuple))
else self.config.sample_size
)
self.tile_latent_min_size = int(sample_size / (2 ** (len(self.config.block_out_channels) - 1)))#æ˜¯æŒ‡tileç»è¿‡encodeä»¥åçš„å¤§å°
self.tile_overlap_factor = 0.25

def _set_gradient_checkpointing(self, module, value=False):
if isinstance(module, (EncoderCausal3D, DecoderCausal3D)):
module.gradient_checkpointing = value

def enable_temporal_tiling(self, use_tiling: bool = True):
self.use_temporal_tiling = use_tiling

def disable_temporal_tiling(self):
self.enable_temporal_tiling(False)

def enable_spatial_tiling(self, use_tiling: bool = True):
self.use_spatial_tiling = use_tiling

def disable_spatial_tiling(self):
self.enable_spatial_tiling(False)

def enable_tiling(self, use_tiling: bool = True):
r"""
Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to
compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow
processing larger videos.
"""
self.enable_spatial_tiling(use_tiling)
self.enable_temporal_tiling(use_tiling)

def disable_tiling(self):
r"""
Disable tiled VAE decoding. If enable_tiling was previously enabled, this method will go back to computing
decoding in one step.
"""
self.disable_spatial_tiling()
self.disable_temporal_tiling()

def enable_slicing(self):
r"""
Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to
compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.
"""
self.use_slicing = True

def disable_slicing(self):
r"""
Disable sliced VAE decoding. If enable_slicing was previously enabled, this method will go back to computing
decoding in one step.
"""
self.use_slicing = False

@property
# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.attn_processors
def attn_processors(self) -> Dict[str, AttentionProcessor]:
r"""
Returns:
dict of attention processors: A dictionary containing all attention processors used in the model with
indexed by its weight name.
"""
# set recursively
processors = {}

def fn_recursive_add_processors(name: str, module: torch.nn.Module, processors: Dict[str, AttentionProcessor]):
if hasattr(module, "get_processor"):
processors[f"{name}.processor"] = module.get_processor(return_deprecated_lora=True)

for sub_name, child in module.named_children():
fn_recursive_add_processors(f"{name}.{sub_name}", child, processors)

return processors

for name, module in self.named_children():
fn_recursive_add_processors(name, module, processors)

return processors

# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.set_attn_processor
def set_attn_processor(
self, processor: Union[AttentionProcessor, Dict[str, AttentionProcessor]], _remove_lora=False
):
r"""
Sets the attention processor to use to compute attention.

Parameters:
processor (dict of AttentionProcessor or only AttentionProcessor):
The instantiated processor class or a dictionary of processor classes that will be set as the processor
for **all** Attention layers.

If processor is a dict, the key needs to define the path to the corresponding cross attention
processor. This is strongly recommended when setting trainable attention processors.

"""
count = len(self.attn_processors.keys())

if isinstance(processor, dict) and len(processor) != count:
raise ValueError(
f"A dict of processors was passed, but the number of processors {len(processor)} does not match the"
f" number of attention layers: {count}. Please make sure to pass {count} processor classes."
)

def fn_recursive_attn_processor(name: str, module: torch.nn.Module, processor):
if hasattr(module, "set_processor"):
if not isinstance(processor, dict):
module.set_processor(processor, _remove_lora=_remove_lora)
else:
module.set_processor(processor.pop(f"{name}.processor"), _remove_lora=_remove_lora)

for sub_name, child in module.named_children():
fn_recursive_attn_processor(f"{name}.{sub_name}", child, processor)

for name, module in self.named_children():
fn_recursive_attn_processor(name, module, processor)

# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.set_default_attn_processor
def set_default_attn_processor(self):
"""
Disables custom attention processors and sets the default attention implementation.
"""
if all(proc.__class__ in ADDED_KV_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
processor = AttnAddedKVProcessor()
elif all(proc.__class__ in CROSS_ATTENTION_PROCESSORS for proc in self.attn_processors.values()):
processor = AttnProcessor()
else:
raise ValueError(
f"Cannot call set_default_attn_processor when attention processors are of type {next(iter(self.attn_processors.values()))}"
)

self.set_attn_processor(processor, _remove_lora=True)

@apply_forward_hook
def encode(
self, x: torch.FloatTensor, return_dict: bool = True
) -> Union[AutoencoderKLOutput, Tuple[DiagonalGaussianDistribution]]:
"""
Encode a batch of images/videos into latents.

Args:
x (torch.FloatTensor): Input batch of images/videos.
return_dict (bool, *optional*, defaults to True):
Whether to return a [~models.autoencoder_kl.AutoencoderKLOutput] instead of a plain tuple.

Returns:
The latent representations of the encoded images/videos. If return_dict is True, a
[~models.autoencoder_kl.AutoencoderKLOutput] is returned, otherwise a plain tuple is returned.
"""
assert len(x.shape) == 5, "The input tensor should have 5 dimensions."
# (B, C, T, H ,W)
print(f"ç‹æ€è¿œx shape: {x.shape}, {self.tile_sample_min_tsize}")

if self.use_temporal_tiling and x.shape[2] > self.tile_sample_min_tsize: #æ—¶é—´å‹ç¼©ä¸­å¯ä»¥åŒ…å«ç©ºé—´å‹ç¼©
print(f"æ—¶ç©ºå‹ç¼©")
return self.temporal_tiled_encode(x, return_dict=return_dict) #åªè¿”å›åˆ†å¸ƒ ä¸è¿”å›æ•°å€¼

if self.use_spatial_tiling and (x.shape[-1] > self.tile_sample_min_size or x.shape[-2] > self.tile_sample_min_size):
print(f"ç©ºé—´å‹ç¼©")
return self.spatial_tiled_encode(x, return_dict=return_dict) #åªè¿”å›åˆ†å¸ƒ ä¸è¿”å›æ•°å€¼

if self.use_slicing and x.shape[0] > 1:#ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œå°†è¾“å…¥åˆ‡ç‰‡
encoded_slices = [self.encoder(x_slice) for x_slice in x.split(1)]
h = torch.cat(encoded_slices)
else:
h = self.encoder(x)

moments = self.quant_conv(h)
posterior = DiagonalGaussianDistribution(moments)

if not return_dict:
return (posterior,)

return AutoencoderKLOutput(latent_dist=posterior)

def _decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:
assert len(z.shape) == 5, "The input tensor should have 5 dimensions."

if self.use_temporal_tiling and z.shape[2] > self.tile_latent_min_tsize:
return self.temporal_tiled_decode(z, return_dict=return_dict)

if self.use_spatial_tiling and (z.shape[-1] > self.tile_latent_min_size or z.shape[-2] > self.tile_latent_min_size):
return self.spatial_tiled_decode(z, return_dict=return_dict)

z = self.post_quant_conv(z) #post_quant_convæ˜¯ä¸€ä¸ª1x1çš„å·ç§¯,ç›´çº¿çº¿æ€§å˜æ¢
dec = self.decoder(z) #decoderæ˜¯ä¸€ä¸ªè§£ç å™¨ï¼Œå°†zè§£ç æˆå›¾ç‰‡ æ­¤æ—¶channelæ˜¯3

if not return_dict:
return (dec,)

return DecoderOutput(sample=dec)
#å®é™…åº”ç”¨çš„decode
@apply_forward_hook
def decode(
self, z: torch.FloatTensor, return_dict: bool = True, generator=None
) -> Union[DecoderOutput, torch.FloatTensor]:
"""
Decode a batch of images/videos.

Args:
z (torch.FloatTensor): Input batch of latent vectors.
return_dict (bool, *optional*, defaults to True):
Whether to return a [~models.vae.DecoderOutput] instead of a plain tuple.

Returns:
[~models.vae.DecoderOutput] or tuple:
If return_dict is True, a [~models.vae.DecoderOutput] is returned, otherwise a plain tuple is
returned.

"""
if self.use_slicing and z.shape[0] > 1:#è¿™æ®µæ˜¯ä¸ºäº†èŠ‚çœå†…å­˜ï¼Œå°†è¾“å…¥åˆ‡ç‰‡
decoded_slices = [self._decode(z_slice).sample for z_slice in z.split(1)]
decoded = torch.cat(decoded_slices)
else:
decoded = self._decode(z).sample

if not return_dict:
return (decoded,)

return DecoderOutput(sample=decoded)
#ä¸Šä¸‹æ··åˆ #B, C, T, H, W
def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:
blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)
for y in range(blend_extent):
b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, :, y, :] * (y / blend_extent)
return b
#å·¦å³æ··åˆ
def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:
blend_extent = min(a.shape[-1], b.shape[-1], blend_extent)
for x in range(blend_extent):
b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, :, x] * (x / blend_extent)
return b

def blend_t(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:
blend_extent = min(a.shape[-3], b.shape[-3], blend_extent)
for x in range(blend_extent):
b[:, :, x, :, :] = a[:, :, -blend_extent + x, :, :] * (1 - x / blend_extent) + b[:, :, x, :, :] * (x / blend_extent)
return b

def spatial_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True, return_moments: bool = False) -> AutoencoderKLOutput:
r"""Encode a batch of images/videos using a tiled encoder.

When this option is enabled, the VAE will split the input tensor into tiles to compute encoding in several
steps. This is useful to keep memory use constant regardless of image/videos size. The end result of tiled encoding is
different from non-tiled encoding because each tile uses a different encoder. To avoid tiling artifacts, the
tiles overlap and are blended together to form a smooth output. You may still see tile-sized changes in the
output, but they should be much less noticeable.

Args:
x (torch.FloatTensor): Input batch of images/videos.
return_dict (bool, *optional*, defaults to True):
Whether or not to return a [~models.autoencoder_kl.AutoencoderKLOutput] instead of a plain tuple.

Returns:
[~models.autoencoder_kl.AutoencoderKLOutput] or tuple:
If return_dict is True, a [~models.autoencoder_kl.AutoencoderKLOutput] is returned, otherwise a plain
tuple is returned.
"""
#æ»‘åŠ¨æ­¥é•¿
overlap_size = int(self.tile_sample_min_size * (1 - self.tile_overlap_factor)) #tile_sample_min_size = 256
#é‡å åŒºåŸŸ
blend_extent = int(self.tile_latent_min_size * self.tile_overlap_factor)
#æœ‰æ•ˆåŒºåŸŸ
row_limit = self.tile_latent_min_size - blend_extent
# Split video into tiles and encode them separately.
rows = []#B, C, T, H, W
for i in range(0, x.shape[-2], overlap_size):
row = []
for j in range(0, x.shape[-1], overlap_size):
tile = x[:, :, :, i: i + self.tile_sample_min_size, j: j + self.tile_sample_min_size]
tile = self.encoder(tile)
tile = self.quant_conv(tile)
row.append(tile)
rows.append(row)
result_rows = []
for i, row in enumerate(rows):
result_row = []
for j, tile in enumerate(row):
# blend the above tile and the left tile
# to the current tile and add the current tile to the result row
if i > 0:
#ä¸Šé¢çš„tileå’Œå½“å‰tileæ··åˆ
tile = self.blend_v(rows[i - 1][j], tile, blend_extent)
if j > 0:
#å·¦è¾¹çš„tileå’Œå½“å‰tileæ··åˆ
tile = self.blend_h(row[j - 1], tile, blend_extent)
result_row.append(tile[:, :, :, :row_limit, :row_limit])
result_rows.append(torch.cat(result_row, dim=-1))

moments = torch.cat(result_rows, dim=-2) #ç»è¿‡ç©ºé—´å‹ç¼©åçš„å°feature map
if return_moments:
return moments

posterior = DiagonalGaussianDistribution(moments)
if not return_dict:
return (posterior,)

return AutoencoderKLOutput(latent_dist=posterior)

def spatial_tiled_decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:
r"""
Decode a batch of images/videos using a tiled decoder.

Args:
z (torch.FloatTensor): Input batch of latent vectors.
return_dict (bool, *optional*, defaults to True):
Whether or not to return a [~models.vae.DecoderOutput] instead of a plain tuple.

Returns:
[~models.vae.DecoderOutput] or tuple:
If return_dict is True, a [~models.vae.DecoderOutput] is returned, otherwise a plain tuple is
returned.
"""
overlap_size = int(self.tile_latent_min_size * (1 - self.tile_overlap_factor))#æ»‘åŠ¨æ­¥é•¿
blend_extent = int(self.tile_sample_min_size * self.tile_overlap_factor)#é‡å åŒºåŸŸ
row_limit = self.tile_sample_min_size - blend_extent#æœ‰æ•ˆåŒºåŸŸ

# Split z into overlapping tiles and decode them separately.
# The tiles have an overlap to avoid seams between tiles.
rows = []
for i in range(0, z.shape[-2], overlap_size):#zçš„ç»´åº¦æ˜¯B, C, T, H, W
row = []
for j in range(0, z.shape[-1], overlap_size):
tile = z[:, :, :, i: i + self.tile_latent_min_size, j: j + self.tile_latent_min_size]
tile = self.post_quant_conv(tile)
decoded = self.decoder(tile)
row.append(decoded)
rows.append(row)
result_rows = []
for i, row in enumerate(rows):
result_row = []
for j, tile in enumerate(row):
# blend the above tile and the left tile
# to the current tile and add the current tile to the result row
# ä»¥ä¸‹æ˜¯è¦è§£å†³åŸå§‹å¤§å°çš„feature mapé‡å çš„é—®é¢˜
if i > 0:
tile = self.blend_v(rows[i - 1][j], tile, blend_extent)
if j > 0:
tile = self.blend_h(row[j - 1], tile, blend_extent)
result_row.append(tile[:, :, :, :row_limit, :row_limit])
result_rows.append(torch.cat(result_row, dim=-1))

dec = torch.cat(result_rows, dim=-2)
if not return_dict:
return (dec,)

return DecoderOutput(sample=dec)

def temporal_tiled_encode(self, x: torch.FloatTensor, return_dict: bool = True) -> AutoencoderKLOutput:

B, C, T, H, W = x.shape
#æ»‘åŠ¨è·ç¦»
overlap_size = int(self.tile_sample_min_tsize * (1 - self.tile_overlap_factor))
#é‡å è·ç¦» æ˜¯åœ¨latentç»´åº¦è®¡ç®—çš„
blend_extent = int(self.tile_latent_min_tsize * self.tile_overlap_factor)
#æœ‰æ•ˆè·ç¦»
t_limit = self.tile_latent_min_tsize - blend_extent

# Split the video into tiles and encode them separately.
row = []
for i in range(0, T, overlap_size):#i + self.tile_sample_min_tsize + 1 é‡Œçš„+1æ˜¯å› ä¸ºtileçš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥æ˜¯ä¸‹ä¸€ä¸ªtileçš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥
tile = x[:, :, i: i + self.tile_sample_min_tsize + 1, :, :] #ç©ºé—´å¿«æ˜¯256*256,æ—¶é—´å¿«æ˜¯64
logger.info(f"ç‹æ€è¿œtile shape: {tile.shape}")
if self.use_spatial_tiling and (tile.shape[-1] > self.tile_sample_min_size or tile.shape[-2] > self.tile_sample_min_size):
tile = self.spatial_tiled_encode(tile, return_moments=True)#åªè¿”å›momentsï¼Œä¸è¿”å›åˆ†å¸ƒ
else:
tile = self.encoder(tile) #ä¼ å…¥VAEæ–‡ä»¶ä¸­çš„encoderï¼Œè¿›è¡Œäº†ä¸€ç³»åˆ—ä¸‹é‡‡æ ·ï¼Œä»¥åŠchannelç»´åº¦æ‰©å®¹
tile = self.quant_conv(tile)
if i > 0:
logger.info(f"ç‹æ€è¿œtile shape: {tile.shape}")
tile = tile[:, :, 1:, :, :] #è¿™é‡Œé¢çš„1æ˜¯å› ä¸ºtileçš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥æ˜¯ä¸‹ä¸€ä¸ªtileçš„ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥
row.append(tile)
result_row = []
for i, tile in enumerate(row):
if i > 0:
tile = self.blend_t(row[i - 1], tile, blend_extent)
result_row.append(tile[:, :, :t_limit, :, :])
else:
result_row.append(tile[:, :, :t_limit + 1, :, :])

moments = torch.cat(result_row, dim=2)

posterior = DiagonalGaussianDistribution(moments)

if not return_dict:
return (posterior,)

return AutoencoderKLOutput(latent_dist=posterior)

def temporal_tiled_decode(self, z: torch.FloatTensor, return_dict: bool = True) -> Union[DecoderOutput, torch.FloatTensor]:
# Split z into overlapping tiles and decode them separately.

B, C, T, H, W = z.shape
overlap_size = int(self.tile_latent_min_tsize * (1 - self.tile_overlap_factor))
blend_extent = int(self.tile_sample_min_tsize * self.tile_overlap_factor)
t_limit = self.tile_sample_min_tsize - blend_extent

row = []
for i in range(0, T, overlap_size):
tile = z[:, :, i: i + self.tile_latent_min_tsize + 1, :, :]
if self.use_spatial_tiling and (tile.shape[-1] > self.tile_latent_min_size or tile.shape[-2] > self.tile_latent_min_size):
decoded = self.spatial_tiled_decode(tile, return_dict=True).sample
else:
tile = self.post_quant_conv(tile)
decoded = self.decoder(tile)
if i > 0:
decoded = decoded[:, :, 1:, :, :]
row.append(decoded)
result_row = []
for i, tile in enumerate(row):
if i > 0:
tile = self.blend_t(row[i - 1], tile, blend_extent)
result_row.append(tile[:, :, :t_limit, :, :])
else:
result_row.append(tile[:, :, :t_limit + 1, :, :])

dec = torch.cat(result_row, dim=2)
if not return_dict:
return (dec,)

return DecoderOutput(sample=dec)

def forward(
self,
sample: torch.FloatTensor,
sample_posterior: bool = False,
return_dict: bool = True,
return_posterior: bool = False,
generator: Optional[torch.Generator] = None,
) -> Union[DecoderOutput2, torch.FloatTensor]:
r"""
Args:
sample (torch.FloatTensor): Input sample.
sample_posterior (bool, *optional*, defaults to False):
Whether to sample from the posterior.
return_dict (bool, *optional*, defaults to True):
Whether or not to return a [DecoderOutput] instead of a plain tuple.
"""
x = sample
# latent_dist=posterioræ˜¯ä¸€ä¸ªåˆ†å¸ƒ
# encodeè¾“å‡ºçš„æ˜¯ä¸€ä¸ªåˆ†å¸ƒ,å…¶ä¸­å‡å€¼ç»´åº¦æ˜¯B, C, T, H, W,å…¶ä¸­Cæ˜¯latent_channels,æ–¹å·®ä¹Ÿæ˜¯è¿™ä¸ªç»´åº¦.
# è¿™å°±ä»£è¡¨vaeå­¦ä¹ åˆ°çš„æ˜¯latent_channelsçº¬åº¦çš„é«˜æ–¯åˆ†å¸ƒ
posterior = self.encode(x).latent_dist
if sample_posterior:
z = posterior.sample(generator=generator) #zçš„å½¢çŠ¶æ˜¯B, C, T, H, W,å…¶ä¸­Cå…·ä½“æ•°å€¼æ˜¯latent_channels=4
else:
z = posterior.mode()
dec = self.decode(z).sample #è¿™æ—¶å€™channelæ˜¯3

if not return_dict:
if return_posterior:
return (dec, posterior)
else:
return (dec,)
if return_posterior:
return DecoderOutput2(sample=dec, posterior=posterior)
else:
return DecoderOutput2(sample=dec)

# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.fuse_qkv_projections
def fuse_qkv_projections(self):
"""
Enables fused QKV projections. For self-attention modules, all projection matrices (i.e., query,
key, value) are fused. For cross-attention modules, key and value projection matrices are fused.

<Tip warning={true}>

This API is ğŸ§ª experimental.

</Tip>
"""
self.original_attn_processors = None

for _, attn_processor in self.attn_processors.items():
if "Added" in str(attn_processor.__class__.__name__):
raise ValueError("fuse_qkv_projections() is not supported for models having added KV projections.")

self.original_attn_processors = self.attn_processors

for module in self.modules():
if isinstance(module, Attention):
module.fuse_projections(fuse=True)

# Copied from diffusers.models.unet_2d_condition.UNet2DConditionModel.unfuse_qkv_projections
def unfuse_qkv_projections(self):
"""Disables the fused QKV projection if enabled.

<Tip warning={true}>

This API is ğŸ§ª experimental.

</Tip>

"""
if self.original_attn_processors is not None:
self.set_attn_processor(self.original_attn_processors)
unet_causal_3d_blocks.pyå†…å®¹æ˜¯:# Copyright 2024 The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ==============================================================================
#
# Modified from diffusers==0.29.2
#
# ==============================================================================

from typing import Optional, Tuple, Union

import torch
import torch.nn.functional as F
from torch import nn
from einops import rearrange

from diffusers.utils import logging
from diffusers.models.activations import get_activation
from diffusers.models.attention_processor import SpatialNorm
from diffusers.models.attention_processor import Attention
from diffusers.models.normalization import AdaGroupNorm
from diffusers.models.normalization import RMSNorm
from loguru import logger as _logger
import sys

logger_ = logging.get_logger(__name__) # pylint: disable=invalid-name

def prepare_causal_attention_mask(n_frame: int, n_hw: int, dtype, device, batch_size: int = None):
seq_len = n_frame * n_hw
mask = torch.full((seq_len, seq_len), float("-inf"), dtype=dtype, device=device)
for i in range(seq_len):
i_frame = i // n_hw
mask[i, : (i_frame + 1) * n_hw] = 0
if batch_size is not None:
mask = mask.unsqueeze(0).expand(batch_size, -1, -1)
return mask


class CausalConv3d(nn.Module):
"""
Implements a causal 3D convolution layer where each position only depends on previous timesteps and current spatial locations.
This maintains temporal causality in video generation tasks.
"""

def __init__(
self,
chan_in,
chan_out,
kernel_size: Union[int, Tuple[int, int, int]],
stride: Union[int, Tuple[int, int, int]] = 1,
dilation: Union[int, Tuple[int, int, int]] = 1,#ä¸å¼€å¯è†¨èƒ€å·ç§¯
pad_mode='replicate',
**kwargs
):
super().__init__()

self.pad_mode = pad_mode
padding = (kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size // 2, kernel_size - 1, 0) # W, H, T
self.time_causal_padding = padding

self.conv = nn.Conv3d(chan_in, chan_out, kernel_size, stride=stride, dilation=dilation, **kwargs)

def forward(self, x):
x = F.pad(x, self.time_causal_padding, mode=self.pad_mode)
return self.conv(x)


class UpsampleCausal3D(nn.Module):
"""
A 3D upsampling layer with an optional convolution.
"""

def __init__(
self,
channels: int,
use_conv: bool = False,
use_conv_transpose: bool = False,
out_channels: Optional[int] = None,
name: str = "conv",
kernel_size: Optional[int] = None,
padding=1,
norm_type=None,
eps=None,
elementwise_affine=None,
bias=True,
interpolate=True,
upsample_factor=(2, 2, 2),
):
super().__init__()
self.channels = channels
self.out_channels = out_channels or channels
self.use_conv = use_conv
self.use_conv_transpose = use_conv_transpose
self.name = name
self.interpolate = interpolate
self.upsample_factor = upsample_factor

if norm_type == "ln_norm":
self.norm = nn.LayerNorm(channels, eps, elementwise_affine)
elif norm_type == "rms_norm":
self.norm = RMSNorm(channels, eps, elementwise_affine)
elif norm_type is None:
self.norm = None
else:
raise ValueError(f"unknown norm_type: {norm_type}")

conv = None
if use_conv_transpose:
raise NotImplementedError
elif use_conv:
if kernel_size is None:
kernel_size = 3
conv = CausalConv3d(self.channels, self.out_channels, kernel_size=kernel_size, bias=bias)

if name == "conv":
self.conv = conv
else:
self.Conv2d_0 = conv

def forward(
self,
hidden_states: torch.FloatTensor,
output_size: Optional[int] = None,
scale: float = 1.0,
) -> torch.FloatTensor:
assert hidden_states.shape[1] == self.channels

if self.norm is not None:
raise NotImplementedError

if self.use_conv_transpose:
return self.conv(hidden_states)

# Cast to float32 to as 'upsample_nearest2d_out_frame' op does not support bfloat16
dtype = hidden_states.dtype
if dtype == torch.bfloat16:
hidden_states = hidden_states.to(torch.float32)

# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984
if hidden_states.shape[0] >= 64:
hidden_states = hidden_states.contiguous()

# if output_size is passed we force the interpolation output
# size and do not make use of scale_factor=2
if self.interpolate:
B, C, T, H, W = hidden_states.shape
first_h, other_h = hidden_states.split((1, T - 1), dim=2)
if output_size is None:
if T > 1:
other_h = F.interpolate(other_h, scale_factor=self.upsample_factor, mode="nearest")

first_h = first_h.squeeze(2)
first_h = F.interpolate(first_h, scale_factor=self.upsample_factor[1:], mode="nearest")
first_h = first_h.unsqueeze(2)
else:
raise NotImplementedError

if T > 1:
hidden_states = torch.cat((first_h, other_h), dim=2)
else:
hidden_states = first_h

# If the input is bfloat16, we cast back to bfloat16
if dtype == torch.bfloat16:
hidden_states = hidden_states.to(dtype)

if self.use_conv:
if self.name == "conv":
hidden_states = self.conv(hidden_states)
else:
hidden_states = self.Conv2d_0(hidden_states)

return hidden_states


class DownsampleCausal3D(nn.Module):
"""
A 3D downsampling layer with an optional convolution.
"""

def __init__(
self,
channels: int,
use_conv: bool = False,
out_channels: Optional[int] = None,
padding: int = 1,
name: str = "conv",
kernel_size=3,
norm_type=None,
eps=None,
elementwise_affine=None,
bias=True,
stride=2,#1pad,2stride,3kernelå¯ä»¥å®ç°å‡åŠ
):
super().__init__()
self.channels = channels
self.out_channels = out_channels or channels
self.use_conv = use_conv
self.padding = padding #æ²¡ç”¨ä¸Šè¿™ä¸ªå‚æ•°
stride = stride
self.name = name
#éƒ½æ˜¯åœ¨é€šé“ç»´åº¦
if norm_type == "ln_norm":
self.norm = nn.LayerNorm(channels, eps, elementwise_affine)
elif norm_type == "rms_norm":
self.norm = RMSNorm(channels, eps, elementwise_affine)
elif norm_type is None:
self.norm = None
else:
raise ValueError(f"unknown norm_type: {norm_type}")

if use_conv:
conv = CausalConv3d(
self.channels, self.out_channels, kernel_size=kernel_size, stride=stride, bias=bias
)
else:
raise NotImplementedError

if name == "conv":
self.Conv2d_0 = conv
self.conv = conv
elif name == "Conv2d_0":
self.conv = conv
else:
self.conv = conv

def forward(self, hidden_states: torch.FloatTensor, scale: float = 1.0) -> torch.FloatTensor:
assert hidden_states.shape[1] == self.channels #[B, C, H, W]

if self.norm is not None:
hidden_states = self.norm(hidden_states.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)

assert hidden_states.shape[1] == self.channels

hidden_states = self.conv(hidden_states)

return hidden_states


class ResnetBlockCausal3D(nn.Module):
r"""
A Resnet block.
"""

def __init__(
self,
*,
in_channels: int,
out_channels: Optional[int] = None,
conv_shortcut: bool = False,
dropout: float = 0.0,
temb_channels: int = 512,
groups: int = 32,
groups_out: Optional[int] = None,
pre_norm: bool = True,
eps: float = 1e-6,
non_linearity: str = "swish",
skip_time_act: bool = False,
# default, scale_shift, ada_group, spatial
time_embedding_norm: str = "default",
kernel: Optional[torch.FloatTensor] = None,
output_scale_factor: float = 1.0,
use_in_shortcut: Optional[bool] = None,
up: bool = False,
down: bool = False,
conv_shortcut_bias: bool = True,
conv_3d_out_channels: Optional[int] = None,
):
super().__init__()
self.pre_norm = pre_norm
self.pre_norm = True
self.in_channels = in_channels
out_channels = in_channels if out_channels is None else out_channels
self.out_channels = out_channels
self.use_conv_shortcut = conv_shortcut
self.up = up
self.down = down
self.output_scale_factor = output_scale_factor
self.time_embedding_norm = time_embedding_norm
self.skip_time_act = skip_time_act

linear_cls = nn.Linear

if groups_out is None:
groups_out = groups

if self.time_embedding_norm == "ada_group":
self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)
elif self.time_embedding_norm == "spatial":
self.norm1 = SpatialNorm(in_channels, temb_channels)
else:
self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)
#ç¬¬ä¸€æ¬¡å·ç§¯ T,H,Wç»´åº¦æ²¡æœ‰å‘ç”Ÿå˜åŒ–,ä»…channelæœ‰å¯èƒ½å˜åŒ–
self.conv1 = CausalConv3d(in_channels, out_channels, kernel_size=3, stride=1)

if temb_channels is not None:
if self.time_embedding_norm == "default":
self.time_emb_proj = linear_cls(temb_channels, out_channels)
elif self.time_embedding_norm == "scale_shift":
self.time_emb_proj = linear_cls(temb_channels, 2 * out_channels)
elif self.time_embedding_norm == "ada_group" or self.time_embedding_norm == "spatial":
self.time_emb_proj = None
else:
raise ValueError(f"Unknown time_embedding_norm : {self.time_embedding_norm} ")
else:
self.time_emb_proj = None

if self.time_embedding_norm == "ada_group":
self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)
elif self.time_embedding_norm == "spatial":
self.norm2 = SpatialNorm(out_channels, temb_channels)
else:
self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)

self.dropout = torch.nn.Dropout(dropout)
conv_3d_out_channels = conv_3d_out_channels or out_channels
#ç¬¬äºŒæ¬¡å·ç§¯ H,Wç»´åº¦æ²¡æœ‰å‘ç”Ÿå˜åŒ–,ä»…channelæœ‰å¯èƒ½å˜åŒ–
self.conv2 = CausalConv3d(out_channels, conv_3d_out_channels, kernel_size=3, stride=1)

self.nonlinearity = get_activation(non_linearity)

self.upsample = self.downsample = None
if self.up:
self.upsample = UpsampleCausal3D(in_channels, use_conv=False)
elif self.down:
self.downsample = DownsampleCausal3D(in_channels, use_conv=False, name="op")

self.use_in_shortcut = self.in_channels != conv_3d_out_channels if use_in_shortcut is None else use_in_shortcut

self.conv_shortcut = None
if self.use_in_shortcut:
self.conv_shortcut = CausalConv3d(
in_channels,
conv_3d_out_channels,
kernel_size=1,
stride=1,
bias=conv_shortcut_bias,
)

def forward(
self,
input_tensor: torch.FloatTensor,
temb: torch.FloatTensor,
scale: float = 1.0,
) -> torch.FloatTensor:
hidden_states = input_tensor

if self.time_embedding_norm == "ada_group" or self.time_embedding_norm == "spatial":
hidden_states = self.norm1(hidden_states, temb)
else:
hidden_states = self.norm1(hidden_states)

hidden_states = self.nonlinearity(hidden_states)

if self.upsample is not None:
# upsample_nearest_nhwc fails with large batch sizes. see https://github.com/huggingface/diffusers/issues/984
if hidden_states.shape[0] >= 64:
input_tensor = input_tensor.contiguous()
hidden_states = hidden_states.contiguous()
input_tensor = (
self.upsample(input_tensor, scale=scale)
)
hidden_states = (
self.upsample(hidden_states, scale=scale)
)
elif self.downsample is not None:
input_tensor = (
self.downsample(input_tensor, scale=scale)
)
hidden_states = (
self.downsample(hidden_states, scale=scale)
)

hidden_states = self.conv1(hidden_states)

if self.time_emb_proj is not None:
if not self.skip_time_act:
temb = self.nonlinearity(temb)
temb = (
self.time_emb_proj(temb, scale)[:, :, None, None]
)

if temb is not None and self.time_embedding_norm == "default":
hidden_states = hidden_states + temb

if self.time_embedding_norm == "ada_group" or self.time_embedding_norm == "spatial":
hidden_states = self.norm2(hidden_states, temb)
else:
hidden_states = self.norm2(hidden_states)

if temb is not None and self.time_embedding_norm == "scale_shift":
scale, shift = torch.chunk(temb, 2, dim=1)
hidden_states = hidden_states * (1 + scale) + shift

hidden_states = self.nonlinearity(hidden_states)

hidden_states = self.dropout(hidden_states)
hidden_states = self.conv2(hidden_states)

if self.conv_shortcut is not None:
input_tensor = (
self.conv_shortcut(input_tensor)
)
#æ®‹å·®
output_tensor = (input_tensor + hidden_states) / self.output_scale_factor

return output_tensor

def get_down_block3d(
down_block_type: str,
num_layers: int,
in_channels: int,
out_channels: int,
temb_channels: int,
add_downsample: bool,
downsample_stride: int,
resnet_eps: float,
resnet_act_fn: str,
transformer_layers_per_block: int = 1,
num_attention_heads: Optional[int] = None,
resnet_groups: Optional[int] = None,
cross_attention_dim: Optional[int] = None,
downsample_padding: Optional[int] = None,
dual_cross_attention: bool = False,
use_linear_projection: bool = False,
only_cross_attention: bool = False,
upcast_attention: bool = False,
resnet_time_scale_shift: str = "default",
attention_type: str = "default",
resnet_skip_time_act: bool = False,
resnet_out_scale_factor: float = 1.0,
cross_attention_norm: Optional[str] = None,
attention_head_dim: Optional[int] = None,
downsample_type: Optional[str] = None,
dropout: float = 0.0,
):
# If attn head dim is not defined, we default it to the number of heads
if attention_head_dim is None:
logger_.warning(
f"It is recommended to provide attention_head_dim when calling get_down_block. Defaulting attention_head_dim to {num_attention_heads}."
)
attention_head_dim = num_attention_heads

down_block_type = down_block_type[7:] if down_block_type.startswith("UNetRes") else down_block_type
if down_block_type == "DownEncoderBlockCausal3D":
return DownEncoderBlockCausal3D(
num_layers=num_layers,
in_channels=in_channels,
out_channels=out_channels,
dropout=dropout,
add_downsample=add_downsample,
downsample_stride=downsample_stride,
resnet_eps=resnet_eps,
resnet_act_fn=resnet_act_fn,
resnet_groups=resnet_groups,
downsample_padding=downsample_padding,
resnet_time_scale_shift=resnet_time_scale_shift,
)
raise ValueError(f"{down_block_type} does not exist.")

def get_up_block3d(
up_block_type: str,
num_layers: int,
in_channels: int,
out_channels: int,
prev_output_channel: int,
temb_channels: int,
add_upsample: bool,
upsample_scale_factor: Tuple,
resnet_eps: float,
resnet_act_fn: str,
resolution_idx: Optional[int] = None,
transformer_layers_per_block: int = 1,
num_attention_heads: Optional[int] = None,
resnet_groups: Optional[int] = None,
cross_attention_dim: Optional[int] = None,
dual_cross_attention: bool = False,
use_linear_projection: bool = False,
only_cross_attention: bool = False,
upcast_attention: bool = False,
resnet_time_scale_shift: str = "default",
attention_type: str = "default",
resnet_skip_time_act: bool = False,
resnet_out_scale_factor: float = 1.0,
cross_attention_norm: Optional[str] = None,
attention_head_dim: Optional[int] = None,
upsample_type: Optional[str] = None,
dropout: float = 0.0,
) -> nn.Module:
# If attn head dim is not defined, we default it to the number of heads
if attention_head_dim is None:
logger_.warning(
f"It is recommended to provide attention_head_dim when calling get_up_block. Defaulting attention_head_dim to {num_attention_heads}."
)
attention_head_dim = num_attention_heads

up_block_type = up_block_type[7:] if up_block_type.startswith("UNetRes") else up_block_type
if up_block_type == "UpDecoderBlockCausal3D":
return UpDecoderBlockCausal3D(
num_layers=num_layers,
in_channels=in_channels,
out_channels=out_channels,
resolution_idx=resolution_idx,
dropout=dropout,
add_upsample=add_upsample,
upsample_scale_factor=upsample_scale_factor,
resnet_eps=resnet_eps,
resnet_act_fn=resnet_act_fn,
resnet_groups=resnet_groups,
resnet_time_scale_shift=resnet_time_scale_shift,
temb_channels=temb_channels,
)
raise ValueError(f"{up_block_type} does not exist.")

class UNetMidBlockCausal3D(nn.Module):
"""
ä¸­é—´æ¨¡å—ï¼ŒåŒ…å« (num_layers + 1) ä¸ª ResnetBlockCausal3D ä»¥åŠå¯é€‰å¤šå¤´ Attentionã€‚
"""

def __init__(
self,
in_channels: int,
temb_channels: int,
dropout: float = 0.0,
num_layers: int = 1,
resnet_eps: float = 1e-6,
resnet_time_scale_shift: str = "default", # default, spatial
resnet_act_fn: str = "swish",
resnet_groups: int = 32,
attn_groups: int = None,
resnet_pre_norm: bool = True,
add_attention: bool = True,
attention_head_dim: int = 1,
output_scale_factor: float = 1.0,
):
super().__init__()
self.add_attention = add_attention
resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)

if attn_groups is None:
attn_groups = resnet_groups if resnet_time_scale_shift == "default" else None

# ç¬¬ 0 ä¸ª Resnet
resnets = [
ResnetBlockCausal3D(
in_channels=in_channels,
out_channels=in_channels,
temb_channels=temb_channels,
eps=resnet_eps,
groups=resnet_groups,
dropout=dropout,
time_embedding_norm=resnet_time_scale_shift,
non_linearity=resnet_act_fn,
output_scale_factor=output_scale_factor,
pre_norm=resnet_pre_norm,
)
]
attentions = []

if attention_head_dim is None:
logger_.warning(
f"It is not recommended to pass attention_head_dim=None. Defaulting attention_head_dim to {in_channels}."
)
attention_head_dim = in_channels

# åé¢ num_layers æ¬¡ï¼šAttention + Resnet
for _ in range(num_layers):
if self.add_attention:
attentions.append(
Attention(
in_channels,
heads=in_channels attention_head_dim,
dim_head=attention_head_dim,
rescale_output_factor=output_scale_factor,
eps=resnet_eps,
norm_num_groups=attn_groups,
spatial_norm_dim=temb_channels if resnet_time_scale_shift == "spatial" else None,
residual_connection=True,
bias=True,
upcast_softmax=True,
_from_deprecated_attn_block=True,
)
)
else:
attentions.append(None)

resnets.append(
ResnetBlockCausal3D(
in_channels=in_channels,
out_channels=in_channels,
temb_channels=temb_channels,
eps=resnet_eps,
groups=resnet_groups,
dropout=dropout,
time_embedding_norm=resnet_time_scale_shift,
non_linearity=resnet_act_fn,
output_scale_factor=output_scale_factor,
pre_norm=resnet_pre_norm,
)
)

self.attentions = nn.ModuleList(attentions)
self.resnets = nn.ModuleList(resnets)

# æ€»è®¡ (num_layers + 1) ä¸ª ResnetBlock
self.num_resblocks = num_layers + 1

# ç”¨äºå­˜å‚¨æ¯ä¸ªresnetæ˜¯å¦åœ¨å‰/ååš pooling/padding
self.resnet_pool_configs = [None] * self.num_resblocks
self.resnet_pad_configs = [None] * self.num_resblocks

def apply_t_ops_config_midblock(self, config: dict):
if not isinstance(config, dict):
return

epb = config.get("enable_t_pool_before_block", [])
epa = config.get("enable_t_pool_after_block", [])

if any(len(lst) != self.num_resblocks for lst in [epb, epa]):
raise ValueError(
f"[UNetMidBlockCausal3D] T-ops config mismatch: we have {self.num_resblocks} ResnetBlock(s), "
f"but got list lengths: {list(map(len, [epb, epa]))}"
)

pool_k = config.get("pool_t_kernel", 2)
pool_s = config.get("pool_t_stride", 2)


for i in range(self.num_resblocks):
self.resnet_pool_configs[i] = {
"enable_before": epb[i],
"enable_after": epa[i],
"kernel": pool_k,
"stride": pool_s
}

def forward(self, hidden_states: torch.FloatTensor, temb: torch.FloatTensor = None) -> torch.FloatTensor:
"""
å…ˆç»è¿‡ç¬¬ 0 ä¸ª ResnetBlockï¼ˆæ—  attentionï¼‰ï¼Œç„¶åå†é‡å¤(Attention + ResnetBlock) num_layersæ¬¡ã€‚
åœ¨æ¯ä¸ª ResnetBlock å‰/åï¼Œæ ¹æ® self.resnet_pool_configs[i] / self.resnet_pad_configs[i] å†³å®šæ˜¯å¦åš replicate pad + avg_pool3dã€‚
"""
for i in range(self.num_resblocks):
# å¦‚æœè¿™æ˜¯ç¬¬ 0 ä¸ª resnetï¼Œå°±æ²¡æœ‰ attentionï¼›å¦åˆ™ i>=1 æ—¶æ³¨æ„ handle attention
if i > 0:
# å…ˆåš attention
attn = self.attentions[i - 1] # ç¬¬ (i-1) ä¸ª attention
if attn is not None:
B, C, T, H, W = hidden_states.shape
hidden_states = rearrange(hidden_states, "b c f h w -> b (f h w) c")
attention_mask = prepare_causal_attention_mask(T, H*W, hidden_states.dtype, hidden_states.device, batch_size=B)
hidden_states = attn(hidden_states, temb=temb, attention_mask=attention_mask)
hidden_states = rearrange(hidden_states, "b (f h w) c -> b c f h w", f=T, h=H, w=W)

pool_conf = self.resnet_pool_configs[i] or {}
if pool_conf.get("enable_before", False):
k, s = pool_conf["kernel"], pool_conf["stride"]
hidden_states = F.pad(hidden_states, pad=(0,0,0,0,k-1,0), mode='replicate')
hidden_states = F.avg_pool3d(hidden_states, kernel_size=(k,1,1), stride=(s,1,1))
# å†åš ResnetBlock
hidden_states = self.resnets[i](hidden_states, temb=temb)

# æ˜¯å¦åœ¨ ResnetBlock ä¹‹ååš pool/pad
if pool_conf.get("enable_after", False):
k, s = pool_conf["kernel"], pool_conf["stride"]
hidden_states = F.pad(hidden_states, pad=(0,0,0,0,k-1,0), mode='replicate')
hidden_states = F.avg_pool3d(hidden_states, kernel_size=(k,1,1), stride=(s,1,1))

return hidden_states

class DownEncoderBlockCausal3D(nn.Module):
def __init__(
self,
in_channels: int,
out_channels: int,
dropout: float = 0.0,
num_layers: int = 1,
resnet_eps: float = 1e-6,
resnet_time_scale_shift: str = "default",
resnet_act_fn: str = "swish",
resnet_groups: int = 32,
resnet_pre_norm: bool = True,
output_scale_factor: float = 1.0,
add_downsample: bool = True,
downsample_stride: int = 2,
downsample_padding: int = 1,
):
super().__init__()
resnets = []

for i in range(num_layers):
in_channels = in_channels if i == 0 else out_channels
resnets.append(
ResnetBlockCausal3D(
in_channels=in_channels,
out_channels=out_channels,
temb_channels=None,
eps=resnet_eps,
groups=resnet_groups,
dropout=dropout,
time_embedding_norm=resnet_time_scale_shift,
non_linearity=resnet_act_fn,
output_scale_factor=output_scale_factor,
pre_norm=resnet_pre_norm,
)
)

self.resnets = nn.ModuleList(resnets)

if add_downsample:
self.downsamplers = nn.ModuleList(
[
DownsampleCausal3D(
out_channels,
use_conv=True,
out_channels=out_channels,
padding=downsample_padding,
name="op",
stride=downsample_stride,
)
]
)
else:
self.downsamplers = None
self.resnet_pool_configs = [None] * num_layers

def apply_t_ops_config(self, block_config: dict):
if "downsample_stride" in block_config:
ds_stride = tuple(block_config["downsample_stride"]) # e.g. [1, 2, 2] -> (1, 2, 2)
if self.downsamplers is not None:
for ds in self.downsamplers:
ds.conv.conv.stride = ds_stride
_logger.info(f"Down Conv Stride updated to {ds_stride}")

num_resnet = len(self.resnets)
epb = block_config.get("enable_t_pool_before_block", [])
epa = block_config.get("enable_t_pool_after_block", [])
if any(len(x) != num_resnet for x in [epb, epa]):
raise ValueError(
f"[DownEncoderBlockCausal3D] config mismatch: expecting {num_resnet} bools in each list."
)

pool_k = block_config.get("pool_t_kernel", 2)
pool_s = block_config.get("pool_t_stride", 2)

for i in range(num_resnet):
pool_conf = {
"enable_before": epb[i],
"enable_after": epa[i],
"kernel": pool_k,
"stride": pool_s
}
self.resnet_pool_configs[i] = pool_conf

def forward(self, hidden_states: torch.FloatTensor, scale: float = 1.0, index: int = None) -> torch.FloatTensor:
for i, resnet in enumerate(self.resnets):
pool_conf = self.resnet_pool_configs[i] or {}
if pool_conf.get("enable_before", False):
k, s = pool_conf["kernel"], pool_conf["stride"]
_logger.info(f"Down Pooling before ResnetBlock: kernel={k}, stride={s}, hidden_states.shape={hidden_states.shape}, layer={i}, index={index}")
padding = (k-1, 0) # ä»…åœ¨æ—¶é—´ç»´åº¦å‰å‘å¡«å…… k-1 ä¸ªåƒç´ 
hidden_states = F.pad(hidden_states, pad=(0, 0, 0, 0, padding[0], padding[1]), mode='replicate')
hidden_states = F.avg_pool3d(hidden_states, kernel_size=(k, 1, 1), stride=(s, 1, 1))

# 2) ResnetBlock
hidden_states = resnet(hidden_states, temb=None, scale=scale)

# 3) pool/pad AFTER
if pool_conf.get("enable_after", False):
k, s = pool_conf["kernel"], pool_conf["stride"]
padding = (k-1, 0) # ä»…åœ¨æ—¶é—´ç»´åº¦å‰å‘å¡«å…… k-1 ä¸ªåƒç´ 
hidden_states = F.pad(hidden_states, pad=(0, 0, 0, 0, padding[0], padding[1]), mode='replicate')
hidden_states = F.avg_pool3d(hidden_states, kernel_size=(k, 1, 1), stride=(s, 1, 1))
_logger.info(f"Down Pooling after ResnetBlock: kernel={k}, stride={s}, hidden_states.shape={hidden_states.shape}, layer={i}, index={index}")

# ä¸‹é‡‡æ ·
if self.downsamplers is not None:
for downsampler in self.downsamplers:
hidden_states = downsampler(hidden_states, scale)

return hidden_states

class UpDecoderBlockCausal3D(nn.Module):
"""
ä¸Šé‡‡æ ·è§£ç å—, åŒ…å« num_layers ä¸ª ResnetBlockCausal3D, ä»¥åŠå¯é€‰ UpsampleCausal3Dã€‚
æ”¯æŒåœ¨æ¯ä¸ª resnet å‰/åæ’å…¥ T ç»´åº¦ pool/padã€‚
"""
def __init__(
self,
in_channels: int,
out_channels: int,
resolution_idx: Optional[int] = None,
dropout: float = 0.0,
num_layers: int = 1,
resnet_eps: float = 1e-6,
resnet_time_scale_shift: str = "default", # default, spatial
resnet_act_fn: str = "swish",
resnet_groups: int = 32,
resnet_pre_norm: bool = True,
output_scale_factor: float = 1.0,
add_upsample: bool = True,
upsample_scale_factor=(2, 2, 2),
temb_channels: Optional[int] = None,
):
super().__init__()
resnets = []
for i in range(num_layers):
block_in_ch = in_channels if i == 0 else out_channels
block = ResnetBlockCausal3D(
in_channels=block_in_ch,
out_channels=out_channels,
temb_channels=temb_channels,
eps=resnet_eps,
groups=resnet_groups,
dropout=dropout,
time_embedding_norm=resnet_time_scale_shift,
non_linearity=resnet_act_fn,
output_scale_factor=output_scale_factor,
pre_norm=resnet_pre_norm,
)
resnets.append(block)
self.resnets = nn.ModuleList(resnets)

if add_upsample:
self.upsamplers = nn.ModuleList([
UpsampleCausal3D(
out_channels,
use_conv=True,
out_channels=out_channels,
upsample_factor=upsample_scale_factor,
)
])
else:
self.upsamplers = None

self.resolution_idx = resolution_idx

# ä¸ºæ¯ä¸ª ResnetBlock å­˜å‚¨ pool/pad é…ç½®
self.resnet_interp_configs = [None] * num_layers
self.resolution_idx = resolution_idx
self.num_layers = num_layers


def apply_t_ops_config(self, block_config: dict):
num_resnet = len(self.resnets)
eib = block_config.get("enable_t_interp_before_block", [])
eia = block_config.get("enable_t_interp_after_block", [])
if any(len(x) != num_resnet for x in [eib, eia]):
raise ValueError(
f"[UpDecoderBlockCausal3D] config mismatch: expecting {num_resnet} bools in each list."
)

interp_scale = block_config.get("interp_t_scale_factor", 2)
interp_mode = block_config.get("interp_mode", "nearest")
for i in range(num_resnet):
interp_conf = {
"enable_before": eib[i],
"enable_after": eia[i],
"scale_factor": interp_scale,
"mode": interp_mode
}
self.resnet_interp_configs[i] = interp_conf

def forward(
self, hidden_states: torch.FloatTensor, temb: Optional[torch.FloatTensor] = None, scale: float = 1.0
) -> torch.FloatTensor:
"""
åœ¨æ¯ä¸ª ResnetBlock å‰/åï¼Œçœ‹æƒ…å†µåš:
- avg_pool3d (pool)
- replicate pad (pad)
- F.interpolate (æ’å€¼)
ç„¶åå†åš block æœ¬èº«ã€‚
"""
for i, resnet in enumerate(self.resnets):
interp_conf = self.resnet_interp_configs[i] or {}

# 1) å…ˆçœ‹çœ‹æœ‰æ²¡æœ‰ "æ’å€¼å‰"ï¼š
if interp_conf.get("enable_before", False):
sc = interp_conf["scale_factor"]
mode = interp_conf["mode"]
# åªå¯¹ time ç»´åº¦æ’å€¼ (dim=2)
# nearestæ’å€¼éå¹³æ»‘, ä½†æ”¯æŒåå‘ä¼ æ’­
if hidden_states.shape[2] > 0:
hidden_states = F.interpolate(
hidden_states,
scale_factor=(sc, 1, 1), # ä»…æ²¿æ—¶é—´ç»´åº¦æ”¾å¤§ sc å€
mode=mode
)
_logger.info(f"Up Interp before ResnetBlock: scale_factor={sc,1,1},hidden_states.shape={hidden_states.shape}, layer={i}")

hidden_states = resnet(hidden_states, temb=temb, scale=scale)

if interp_conf.get("enable_after", False):
sc = interp_conf["scale_factor"]
mode = interp_conf["mode"]
if hidden_states.shape[2] > 0:
hidden_states = F.interpolate(
hidden_states,
scale_factor=(sc, 1, 1),
mode=mode
)
_logger.info(f"Up Interp before ResnetBlock: scale_factor={sc,1,1},hidden_states.shape={hidden_states.shape}, layer={i}")
# upsample (åŸé€»è¾‘)
if self.upsamplers is not None:
for upsampler in self.upsamplers:
hidden_states = upsampler(hidden_states)

return hidden_states
vae.pyå†…å®¹æ˜¯:from dataclasses import dataclass
from typing import Optional, Tuple

import numpy as np
import torch
import torch.nn as nn
from loguru import logger
from diffusers.utils import BaseOutput, is_torch_version
from diffusers.utils.torch_utils import randn_tensor
from diffusers.models.attention_processor import SpatialNorm
from .unet_causal_3d_blocks import (
CausalConv3d,
UNetMidBlockCausal3D,
get_down_block3d,
get_up_block3d,
)


@dataclass
class DecoderOutput(BaseOutput):
r"""
Output of decoding method.

Args:
sample (torch.FloatTensor of shape (batch_size, num_channels, height, width)):
The decoded output sample from the last layer of the model.
"""

sample: torch.FloatTensor


class EncoderCausal3D(nn.Module):
r"""
The EncoderCausal3D layer of a variational autoencoder that encodes its input into a latent representation.
"""

def __init__(
self,
in_channels: int = 3,
out_channels: int = 3,
down_block_types: Tuple[str, ...] = ("DownEncoderBlockCausal3D",),
block_out_channels: Tuple[int, ...] = (64,),
layers_per_block: int = 2,
norm_num_groups: int = 32,
act_fn: str = "silu",
double_z: bool = True,
mid_block_add_attention=True,
time_compression_ratio: int = 4,
spatial_compression_ratio: int = 8,
):
super().__init__()
self.layers_per_block = layers_per_block

self.conv_in = CausalConv3d(in_channels, block_out_channels[0], kernel_size=3, stride=1)
self.mid_block = None
self.down_blocks = nn.ModuleList([])

# down
output_channel = block_out_channels[0]
for i, down_block_type in enumerate(down_block_types):
input_channel = output_channel#æ¯ä¸ªå—çš„è¾“å…¥é€šé“æ˜¯ä¸Šä¸€ä¸ªå—çš„è¾“å‡ºé€šé“ã€‚
output_channel = block_out_channels[i]
is_final_block = i == len(block_out_channels) - 1
#ä¸‹é‡‡æ ·çš„å±‚æ•°
num_spatial_downsample_layers = int(np.log2(spatial_compression_ratio))
num_time_downsample_layers = int(np.log2(time_compression_ratio))

if time_compression_ratio == 4:
#ç©ºé—´ä¸‹é‡‡æ ·å±‚å±‚æœ‰
add_spatial_downsample = bool(i < num_spatial_downsample_layers)
#æ—¶é—´ä¸‹é‡‡æ ·é€šå¸¸å®‰æ’åœ¨ç¼–ç å™¨çš„åå‡ å±‚
add_time_downsample = bool(
i >= (len(block_out_channels) - 1 - num_time_downsample_layers)
and not is_final_block
)
else:
raise ValueError(f"Unsupported time_compression_ratio: {time_compression_ratio}.")

downsample_stride_HW = (2, 2) if add_spatial_downsample else (1, 1)
downsample_stride_T = (2,) if add_time_downsample else (1,)
downsample_stride = tuple(downsample_stride_T + downsample_stride_HW)
down_block = get_down_block3d(
down_block_type,
num_layers=self.layers_per_block,
in_channels=input_channel,
out_channels=output_channel,
add_downsample=bool(add_spatial_downsample or add_time_downsample),#æœ€åè¿›è¡Œä¸‹ç©ºé—´ä¸‹é‡‡æ ·,ä»¥åŠç©ºé—´ä¸‹é‡‡æ ·åçš„æ—¶é—´ä¸‹é‡‡æ ·,é€šè¿‡ä¼ å…¥strideçš„tupleè¿›è¡Œæ§åˆ¶
downsample_stride=downsample_stride,
resnet_eps=1e-6,
downsample_padding=0,#æ²¡ç”¨ä¸Šè¿™ä¸ªå‚æ•°
resnet_act_fn=act_fn,
resnet_groups=norm_num_groups,
attention_head_dim=output_channel,
temb_channels=None,
)
self.down_blocks.append(down_block)

# mid
self.mid_block = UNetMidBlockCausal3D(
in_channels=block_out_channels[-1],
resnet_eps=1e-6,
resnet_act_fn=act_fn,
output_scale_factor=1,
resnet_time_scale_shift="default",
attention_head_dim=block_out_channels[-1],
resnet_groups=norm_num_groups,
temb_channels=None,
add_attention=mid_block_add_attention,
)

# out
self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-6)
self.conv_act = nn.SiLU()

conv_out_channels = 2 * out_channels if double_z else out_channels
self.conv_out = CausalConv3d(block_out_channels[-1], conv_out_channels, kernel_size=3) #strideè¿˜æ˜¯1

def forward(self, sample: torch.FloatTensor) -> torch.FloatTensor:
r"""The forward method of the EncoderCausal3D class."""
assert len(sample.shape) == 5, "The input tensor should have 5 dimensions"

sample = self.conv_in(sample) #ä»…channelçº¬åº¦å‘ç”Ÿå˜åŒ–,in_channels -> block_out_channels[0]

# down
for i, down_block in enumerate(self.down_blocks): #æ”¹æˆ
sample = down_block(sample, index=i)

# middle
sample = self.mid_block(sample)

# post-process
sample = self.conv_norm_out(sample) #channelç»´åº¦GroupNorm
sample = self.conv_act(sample)
sample = self.conv_out(sample) #ä»…channelçº¬åº¦å‘ç”Ÿå˜åŒ–,block_out_channels[-1] -> conv_out_channels

return sample


class DecoderCausal3D(nn.Module):
r"""
The DecoderCausal3D layer of a variational autoencoder that decodes its latent representation into an output sample.
"""

def __init__(
self,
in_channels: int = 3,
out_channels: int = 3,
up_block_types: Tuple[str, ...] = ("UpDecoderBlockCausal3D",),
block_out_channels: Tuple[int, ...] = (64,),
layers_per_block: int = 2,
norm_num_groups: int = 32,
act_fn: str = "silu",
norm_type: str = "group", # group, spatial
mid_block_add_attention=True,
time_compression_ratio: int = 4,
spatial_compression_ratio: int = 8,
):
super().__init__()
self.layers_per_block = layers_per_block

self.conv_in = CausalConv3d(in_channels, block_out_channels[-1], kernel_size=3, stride=1)
self.mid_block = None
self.up_blocks = nn.ModuleList([])

temb_channels = in_channels if norm_type == "spatial" else None

# mid
self.mid_block = UNetMidBlockCausal3D(
in_channels=block_out_channels[-1],
resnet_eps=1e-6,
resnet_act_fn=act_fn,
output_scale_factor=1,
resnet_time_scale_shift="default" if norm_type == "group" else norm_type,
attention_head_dim=block_out_channels[-1],
resnet_groups=norm_num_groups,
temb_channels=temb_channels, #ä¸encoderçš„åŒºåˆ« ä¹Ÿæ˜¯é¢å¤–ä¼ å…¥äº†temb_channels è€Œåƒç´ çš„channelä¸€æ · ,ä¸»è¦æ˜¯åº”ç”¨äºResnetBlockCausal3Dä¸­,å·ç§¯çš„ä¹‹å‰ä¸ä¹‹å
add_attention=mid_block_add_attention,
)

# up
reversed_block_out_channels = list(reversed(block_out_channels))
output_channel = reversed_block_out_channels[0]
for i, up_block_type in enumerate(up_block_types):
prev_output_channel = output_channel
output_channel = reversed_block_out_channels[i]
is_final_block = i == len(block_out_channels) - 1
num_spatial_upsample_layers = int(np.log2(spatial_compression_ratio))
num_time_upsample_layers = int(np.log2(time_compression_ratio))

if time_compression_ratio == 4:
add_spatial_upsample = bool(i < num_spatial_upsample_layers)
add_time_upsample = bool(
i >= len(block_out_channels) - 1 - num_time_upsample_layers
and not is_final_block
)
else:
raise ValueError(f"Unsupported time_compression_ratio: {time_compression_ratio}.")

upsample_scale_factor_HW = (2, 2) if add_spatial_upsample else (1, 1)
upsample_scale_factor_T = (2,) if add_time_upsample else (1,)
upsample_scale_factor = tuple(upsample_scale_factor_T + upsample_scale_factor_HW)
up_block = get_up_block3d(
up_block_type,
num_layers=self.layers_per_block + 1, #é¢å¤–å¤šäº†ä¸€å±‚ encoderä¸­çš„layeræ˜¯æŒ‡ResnetBlockCausal3Dä¸ªæ•° è¿™é‡ŒåŒæ · ä½†æ˜¯å¤šäº†ä¸€å±‚
in_channels=prev_output_channel,
out_channels=output_channel,
prev_output_channel=None,
add_upsample=bool(add_spatial_upsample or add_time_upsample),
upsample_scale_factor=upsample_scale_factor, #è·Ÿdownçš„strideæ˜¯ä¸€ä¸ªå‚æ•°
resnet_eps=1e-6,
resnet_act_fn=act_fn,
resnet_groups=norm_num_groups,
attention_head_dim=output_channel,
temb_channels=temb_channels, #ä¼ å…¥äº†å®é™…çš„æ•°å€¼ temb ç”¨äºå¯¹æ¿€æ´»å€¼è¿›è¡Œnorm åˆå¤šç§normæ–¹æ³• 1.scale,shift ada 2. ç›´æ¥ç›¸åŠ  default ç­‰ç­‰ è¿™ä¸ªæ¯”è¾ƒå¤š ä¸»è¦ç›®çš„å°±æ˜¯norm
resnet_time_scale_shift=norm_type,
)
self.up_blocks.append(up_block)
prev_output_channel = output_channel

# out
if norm_type == "spatial":
self.conv_norm_out = SpatialNorm(block_out_channels[0], temb_channels)
else:
self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=norm_num_groups, eps=1e-6)
self.conv_act = nn.SiLU()
self.conv_out = CausalConv3d(block_out_channels[0], out_channels, kernel_size=3)

self.gradient_checkpointing = False

def forward(
self,
sample: torch.FloatTensor,
latent_embeds: Optional[torch.FloatTensor] = None,
) -> torch.FloatTensor:
r"""The forward method of the DecoderCausal3D class."""
assert len(sample.shape) == 5, "The input tensor should have 5 dimensions."

sample = self.conv_in(sample)

upscale_dtype = next(iter(self.up_blocks.parameters())).dtype
if self.training and self.gradient_checkpointing:

def create_custom_forward(module):
def custom_forward(*inputs):
return module(*inputs)

return custom_forward

if is_torch_version(">=", "1.11.0"):
# middle
sample = torch.utils.checkpoint.checkpoint(
create_custom_forward(self.mid_block),
sample,
latent_embeds,
use_reentrant=False,
)
sample = sample.to(upscale_dtype)

# up
for up_block in self.up_blocks:
sample = torch.utils.checkpoint.checkpoint(
create_custom_forward(up_block),
sample,
latent_embeds,
use_reentrant=False,
)
else:
# middle
sample = torch.utils.checkpoint.checkpoint(
create_custom_forward(self.mid_block), sample, latent_embeds
)
sample = sample.to(upscale_dtype)

# up
for up_block in self.up_blocks:
sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds)
else:
# middle
sample = self.mid_block(sample, latent_embeds)
sample = sample.to(upscale_dtype)

# up
for up_block in self.up_blocks:
sample = up_block(sample, latent_embeds)

# post-process
if latent_embeds is None:
sample = self.conv_norm_out(sample)
else:
sample = self.conv_norm_out(sample, latent_embeds)
sample = self.conv_act(sample)
sample = self.conv_out(sample)

return sample


class DiagonalGaussianDistribution(object):
def __init__(self, parameters: torch.Tensor, deterministic: bool = False):
if parameters.ndim == 3:
dim = 2 # (B, L, C)
elif parameters.ndim == 5 or parameters.ndim == 4:
dim = 1 # (B, C, T, H ,W) / (B, C, H, W)
else:
raise NotImplementedError
self.parameters = parameters
self.mean, self.logvar = torch.chunk(parameters, 2, dim=dim)
self.logvar = torch.clamp(self.logvar, -30.0, 20.0) #é¿å…å¯¹æ•°æ–¹å·®çš„å€¼è¿‡å°æˆ–è¿‡å¤§å¯¼è‡´æ•°å€¼ä¸ç¨³å®š
self.deterministic = deterministic
self.std = torch.exp(0.5 * self.logvar)
self.var = torch.exp(self.logvar)
if self.deterministic:
self.var = self.std = torch.zeros_like(
self.mean, device=self.parameters.device, dtype=self.parameters.dtype
)
#ä»æŒ‡å®šåˆ†å¸ƒä¸Šè¿›è¡Œé‡‡æ ·
def sample(self, generator: Optional[torch.Generator] = None) -> torch.FloatTensor:
# make sure sample is on the same device as the parameters and has same dtype
sample = randn_tensor(
self.mean.shape,
generator=generator,
device=self.parameters.device,
dtype=self.parameters.dtype,
)
x = self.mean + self.std * sample
return x

def kl(self, other: "DiagonalGaussianDistribution" = None) -> torch.Tensor:
if self.deterministic:
return torch.Tensor([0.0])
else:
reduce_dim = list(range(1, self.mean.ndim))
if other is None: #è·Ÿæ ‡å‡†æ­£æ€åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒ
return 0.5 * torch.sum(
torch.pow(self.mean, 2) + self.var - 1.0 - self.logvar,
dim=reduce_dim,
)
else:#è·Ÿå·²çŸ¥åˆ†å¸ƒè¿›è¡Œæ¯”è¾ƒ å¤šç»´é«˜æ–¯ meanå’Œstdéƒ½æ˜¯å¤šç»´åº¦ è¦æ±‚é™¤äº†batchä¹‹å¤–,t,h,w,cæ±‚å’Œ
return 0.5 * torch.sum(
torch.pow(self.mean - other.mean, 2) / other.var
+ self.var / other.var
- 1.0
- self.logvar
+ other.logvar,
dim=reduce_dim,
)

def nll(self, sample: torch.Tensor, dims: Tuple[int, ...] = [1, 2, 3]) -> torch.Tensor:
if self.deterministic:
return torch.Tensor([0.0])
logtwopi = np.log(2.0 * np.pi)
return 0.5 * torch.sum(
logtwopi + self.logvar +
torch.pow(sample - self.mean, 2) / self.var,
dim=dims,
)

def mode(self) -> torch.Tensor:
return self.mean
è¯»å–çš„jsonæ–‡ä»¶æ˜¯:t_ops_config.json:{
"encoder": {
"down_blocks": [
{
"block_type": "DownEncoderBlockCausal3D",
"block_index": 0,
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false],
"downsample_stride": [1, 2, 2]
},
{
"block_type": "DownEncoderBlockCausal3D",
"block_index": 1,
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false],
"downsample_stride": [2, 2, 2]
},
{
"block_type": "DownEncoderBlockCausal3D",
"block_index": 2,
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false],
"downsample_stride": [2, 2, 2]
},
{
"block_type": "DownEncoderBlockCausal3D",
"block_index": 3,
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false],
"downsample_stride": [1, 1, 1]
}
],
"mid_block": {
"mid_block_type": "UNetMidBlockCausal3D",
"pool_t_kernel": 3,
"pool_t_stride": 2,
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false]
}
},

"decoder": {
"up_blocks": [
{
"block_type": "UpDecoderBlockCausal3D",
"block_index": 0,
"enable_t_interp_before_block": [false, false, false],
"enable_t_interp_after_block": [false, false, false],
"interp_t_scale_factor": 2,
"interp_mode": "nearest"
},
{
"block_type": "UpDecoderBlockCausal3D",
"block_index": 1,
"enable_t_interp_before_block": [false, false, false],
"enable_t_interp_after_block": [false, false, false],
"interp_t_scale_factor": 2,
"interp_mode": "nearest"
},
{
"block_type": "UpDecoderBlockCausal3D",
"block_index": 2,
"enable_t_interp_before_block": [false, false, false],
"enable_t_interp_after_block": [false, false, false],
"interp_t_scale_factor": 2,
"interp_mode": "nearest"
},
{
"block_type": "UpDecoderBlockCausal3D",
"block_index": 3,
"enable_t_interp_before_block": [false, false, false],
"enable_t_interp_after_block": [false, false, false],
"interp_t_scale_factor": 2,
"interp_mode": "nearest"
}

],
"mid_block": {
"mid_block_type": "UNetMidBlockCausal3D",
"enable_t_pool_before_block": [false, false],
"enable_t_pool_after_block": [false, false]
}
}
}
æˆ‘æƒ³å†æ¬¡åŸºç¡€ä¸Šä¿®æ”¹ä»£ç ,åœ¨autoencoder_kl_causal_3d.pyä¸­æ„å»ºä¸€ä¸ªç±»,è¿™ç±»é‡Œé¢æœ‰å¤šç§æ–¹å¼å¯ä»¥è®¡ç®—temporal_tiled_encodeå‡½æ•°ä¸­æ¯ä¸ªåœ¨frameçº¬åº¦åˆ‡åˆ†åçš„tileçš„ä¿¡æ¯é‡,æ¯”å¦‚æˆ‘å¯ä»¥ç”¨ffprobeè®¡ç®—æ¯ä¸ªtileçš„å¹³å‡ç ç‡(ä½ å…ˆå®ç°è¿™ä¸ªæ–¹æ¡ˆ,é™¤æ­¤ä¹‹å¤–  ä½ å¯ä»¥å†å®ç°ä¸¤ç§æ–¹æ¡ˆ,ç»™æˆ‘ç•™ä¸ªdef passè¿™ç§å‡½æ•° æ–¹ä¾¿æˆ‘åç»­è‡ªåŠ¨ä»¥è®¡ç®—ä¿¡æ¯é‡çš„æ–¹æ³•,è¿™ä¸ªå‡½æ•°è¦æ±‚è¯»å–tileè¾“å‡ºå±äºå“ªä¸ªæ¡£),ç„¶åæ ¹æ®ç ç‡æ˜¯å±äº1000ä»¥å†…,è¿˜æ˜¯1000-2000,2000ä»¥ä¸Šåˆ†ä¸ºä¸‰æ¡£,æ¥å†³å®šå½“å‰tileçš„å‹ç¼©ç‡,0~1000å‹ç¼©ç‡æ˜¯4å€,1000~2000æ˜¯2å€,2000ä»¥ä¸Šä¸å‹ç¼©.å¯¹åº”çš„ä½ å°±å¯ä»¥loadä¸åŒçš„jsonæ–‡ä»¶,4å€çš„jsonæ–‡ä»¶è·¯æ˜¯/home/hanling/HunyuanVideo_efficiency/analysis/config_stride2_json/exp_262.json,2å€çš„jsonæ–‡ä»¶è·¯å¾„æ˜¯:/home/hanling/HunyuanVideo_efficiency/analysis/config_stride_json/exp_20.json,ä½ è€ƒè€ƒè™‘ä¸‹ä¸åŒå‹ç¼©æ˜¯ä¸æ˜¯éœ€è¦ä¿®æ”¹tiled_encode,decode,blendç­‰å‡½æ•° ä»¥æ±‚æ‹¼æ¥ä¸Šä¸è¦å‡ºä»€ä¹ˆé—®é¢˜