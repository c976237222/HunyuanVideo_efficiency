import os
import torch
import cv2
import argparse
import torchvision.transforms as transforms
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed

# ğŸ› ï¸ å‘½ä»¤è¡Œå‚æ•°é…ç½®
parser = argparse.ArgumentParser(description='è§†é¢‘å¤„ç†ä¸Tensorè½¬æ¢')
parser.add_argument('--target_height', type=int, default=None,
                    help='ç›®æ ‡å‚ç›´åˆ†è¾¨ç‡ï¼ˆå¦‚720/360/240ï¼‰ï¼Œé»˜è®¤Noneè¡¨ç¤ºä¿æŒåŸå§‹å°ºå¯¸')
parser.add_argument('--start_frame', type=int, default=None,
                    help='èµ·å§‹å¸§ï¼ˆåŒ…å«ï¼‰ï¼Œé»˜è®¤ä»ç¬¬0å¸§å¼€å§‹')
parser.add_argument('--end_frame', type=int, default=None,
                    help='ç»“æŸå¸§ï¼ˆä¸åŒ…å«ï¼‰ï¼Œé»˜è®¤å¤„ç†åˆ°æœ€åä¸€å¸§')
args = parser.parse_args()

# ğŸ“‚ åŠ¨æ€ç”Ÿæˆè¾“å‡ºè·¯å¾„
base_dir = "/home/hanling/HunyuanVideo_efficiency/video_data"
resolution_tag = f"{args.target_height}p" if args.target_height else "original"

video_dir = os.path.join(base_dir, "large_motion1")
output_video_dir = os.path.join(base_dir, f"large_motion1_{resolution_tag}_videos")
output_tensor_dir = os.path.join(base_dir, f"large_motion1_{resolution_tag}_tensors")

# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
os.makedirs(output_video_dir, exist_ok=True)
os.makedirs(output_tensor_dir, exist_ok=True)

# ğŸ¥ è§†é¢‘å¤„ç†å‡½æ•°
def process_video(video_file):
    """å®Œæ•´çš„è§†é¢‘å¤„ç†æµæ°´çº¿"""
    input_path = os.path.join(video_dir, video_file)
    output_path = os.path.join(output_video_dir, video_file)
    tensor_path = os.path.join(output_tensor_dir, video_file.replace(".mp4", ".pt"))
    
    # ===== é˜¶æ®µ1ï¼šåˆ†è¾¨ç‡å¤„ç† =====
    if args.target_height:
        # è¯»å–åŸå§‹è§†é¢‘å‚æ•°
        cap = cv2.VideoCapture(input_path)
        orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        # è®¡ç®—æ–°å°ºå¯¸ï¼ˆä¿æŒå®½é«˜æ¯”ï¼‰
        new_height = args.target_height
        new_width = int(orig_width * (new_height / orig_height))
        new_width = new_width // 2 * 2  # å®½åº¦è°ƒæ•´ä¸ºå¶æ•°
        
        # åˆå§‹åŒ–è§†é¢‘å†™å…¥å™¨
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (new_width, new_height))
        
        current_frame = 0
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break
            
            # ğŸ”¥ æ–°å¢å¸§èŒƒå›´è¿‡æ»¤ï¼ˆä½œç”¨äºè§†é¢‘ä¿å­˜ï¼‰
            if args.start_frame and current_frame < args.start_frame:
                current_frame += 1
                continue
            if args.end_frame and current_frame >= args.end_frame:
                break
            
            # è°ƒæ•´åˆ†è¾¨ç‡å¹¶å†™å…¥
            resized_frame = cv2.resize(frame, (new_width, new_height))
            out.write(resized_frame)
            current_frame += 1
        
        cap.release()
        out.release()
        video_path = output_path
        final_size = (new_width, new_height)
    else:
        # ç›´æ¥ä½¿ç”¨åŸå§‹è§†é¢‘
        video_path = input_path
        cap = cv2.VideoCapture(video_path)
        final_size = (int(cap.get(3)), int(cap.get(4)))  # (width, height)
        cap.release()

    # ===== é˜¶æ®µ2ï¼šTensorè½¬æ¢ =====
    cap = cv2.VideoCapture(video_path)
    frames = []
    current_frame = 0
    
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret: break
        
        # å¸§èŒƒå›´è¿‡æ»¤
        if args.start_frame and current_frame < args.start_frame:
            current_frame += 1
            continue
        if args.end_frame and current_frame >= args.end_frame:
            break
        
        # è½¬æ¢ä¸ºTensorå¹¶æ ‡å‡†åŒ–
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        tensor_frame = transforms.ToTensor()(frame)  # [0,1]èŒƒå›´
        frames.append(tensor_frame)
        current_frame += 1
    
    cap.release()
    
    if not frames:
        return f"âš ï¸ è·³è¿‡ç©ºè§†é¢‘ï¼š{video_file}"
    
    # ç»„åˆTensor
    video_tensor = torch.stack(frames)          # (T, C, H, W)
    video_tensor = video_tensor.permute(1, 0, 2, 3)  # (C, T, H, W)
    video_tensor = 2 * video_tensor - 1         # [-1, 1]èŒƒå›´
    
    # ä¿å­˜ç»“æœ
    torch.save(video_tensor, tensor_path)
    
    # è¿”å›å¤„ç†ä¿¡æ¯
    C, T, H, W = video_tensor.shape
    return (
        f"å¤„ç†æˆåŠŸï¼š{video_file}\n"
        f"â”œâ”€ è§†é¢‘å°ºå¯¸ï¼š{final_size[0]}x{final_size[1]}\n"
        f"â”œâ”€ Tensorå½¢çŠ¶ï¼šC={C}, T={T}, H={H}, W={W}\n"
        f"â””â”€ æ•°å€¼èŒƒå›´ï¼š[-1, 1] (dtype: {video_tensor.dtype})"
    )

# âš™ï¸ å¤šçº¿ç¨‹æ‰§è¡Œ
if __name__ == "__main__":
    video_files = [f for f in os.listdir(video_dir) if f.endswith(".mp4")]
    
    with ThreadPoolExecutor(max_workers=50) as executor:
        futures = {executor.submit(process_video, f): f for f in video_files}
        
        for future in tqdm(as_completed(futures), total=len(video_files)):
            result = future.result()
            tqdm.write("\n" + "="*50)
            tqdm.write(result)

    print("âœ… æ‰€æœ‰è§†é¢‘å¤„ç†å®Œæˆï¼")